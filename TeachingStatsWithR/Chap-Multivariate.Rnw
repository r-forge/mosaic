\chapter{Multivariate Statistics -- Early?} 
\label{chap:multivariate-early}


\SweaveOpts{
tidy=TRUE,
dev=pdf,
fig.path=figures/fig-,  
fig.width=4, fig.height=2.5,
fig.align=center,
fig.show=hold,
comment=NA
}

<<setup,echo=FALSE,message=FALSE,results=hide>>=
source('setup.R')
trellis.par.set(fontsize=list(text=10))
@ 

If you are of a certain age, you may remember the 1960s television show {\em
  Lost in Space}.  One of the characters, the Robot, was often
assigned guard duty.  Robot would march back and forth, just like a
soldier on guard duty.

But why?  Soldiers are ordered to march back and forth so that they
won't fall asleep; walking forces them to maintain a certain attention
to their duty.  Robot has no such need; his sensors and circuits can reliably
detect intruders without the marching.  Of course, the television
viewers wouldn't know this about robots.  Using the robot in the
manner of a soldier was a way to introduce new technology to people
with old mind-sets.

Now fast forward from the television of the 1960s to the classroom of
the 21st century.  Students have computers.  They use statistical
software packages to do calculations.  But what calculations are they
doing?  Sample means, sample proportions, differences between means.

This is Robot walking back and forth.  A way to use new technology with
old mind-sets.  But it's the professors, not the students, with the
old mind-sets.  The students are open to new things.  They don't need
to know that, once upon a time, it was a feat to get a machine to add
up a column of numbers.

We professors were educated at a time when the tools for inverting a
matrix were paper and pencil, when doing least squares problem involved
a strange thing called a ``pseudo-inverse'' that you might learn in
your fourth or fifth semester of university-level mathematics.  But, now, least squares
problems are no more difficult or time consuming to the human than
square roots or addition.  We just have to learn to use the tools in
the right way.  

Or, rather, we have to show our students what are the
basic operations that are important for statistical reasoning in an
age of modern computation.  Not marching back and forth, like robot
soldiers, computing sums of columns of numbers, but thinking about how to model the
potentially rich interplay among multiple variables.

The standard approach to introductory statistics is based on a few
simple operations: adding, squaring, and square-rooting to finding means 
and standard deviations; 
counting to find proportions and medians/quantiles.  These operations are
(supposed to be) familiar to students from high-school mathematics.

Here we'll examine the consequences of adding in a new basic operation
that is not in the traditional high-school curriculum, but which is
actually quite consistent with the ``Common Core'' curriculum being
introduced by many U.S. states.\cite{common-core-2010}


The operation is fitting multivariate linear models.  Modern software
makes it no more arduous than finding a mean or standard deviation.
Our emphasis here will be on how to introduce the conceptual
foundations --- using just high-school math and simple extensions
largely specified already in the ``Common Core'' --- that make the concept
of modeling accessible.

There are three important pedagogical advantages to using multivariate linear models
as a foundation for statistics:
\begin{enumerate}
  \item It provides a logically coherent framework that unifies many
    of the disparate techniques found in introductory statistics books.
  \item It allows the very important ideas of confounding and
    covariates to be introduced in an integrated way, showing students
    not only the perils of confounding, but what to do about it (and
    when you can't do anything).
  \item It allows the examples used in classes to be drawn from a
    richer set of situations, and provides scope for students to
    express their creativity and insight.  This can increase student
    motivation.
\end{enumerate}
Of course, it's also important that modern statistical work is already
and increasingly shaped by multivariate methods.  For an example of
how over the last few decades statistical methods used in the
literature have shifted away from the curriculum of the traditional,
non-multivariate introductory course, see the review of statistics
practices in the \emph{New England Journal of Medicine}. \cite{switzer-horton-2005}

\section{The Mathematical Foundations}

A standard part of the high-school curriculum is the equation of a
straight line: $y = m x + b$.  Many students will recognize that $m$
is a slope and $b$ is the $y$-intercept.  

It's helpful to move them a bit beyond this:
\begin{itemize}
\item Emphasize the ``function'' concept.  A function, of course, is a
  relationship between an input and an output.  Generalize this, so
  that students are comfortable with using names other than $x$ as the
  input and
  $y$ as the output.  For example, 
  $$\mbox{height} = f( \mbox{age} ) = 3\ \mbox{age} + 20$$.
  
\item Introduce the idea of functions of non-numeric variables, for example:
$$\mbox{height} = g( \mbox{sex} ) = \left\{ \begin{array}{ll}
      67 & \mbox{for sex $=$ female}\\
      71 & \mbox{for sex $=$ male}\\
      \end{array}\right.$$
   
\item Generalize the idea of a function to include having more than
  one input, for instance

$$\mbox{height} = h( \mbox{age}, \mbox{sex} ) = -2\ \mbox{age} + 
      \left\{ \begin{array}{ll}
      67 & \mbox{for sex $=$ female}\\
      71 & \mbox{for sex $=$ male}\\
      \end{array}\right.$$

\item  De-program your students from thinking that there is one, and
  only one, correct formula for a relationship.  Introduce the idea of a function as a
  description of a relationship and that there can be different descriptions of the same
  relationship, all of which can be right in some ways.  Different
  descriptions can include some details and exclude others. Such
  descriptions are called ``models.''  The above models
  $f(\mbox{age})$, and $g(\mbox{sex})$ and $h(\mbox{age}, \mbox{sex})$
  give different outputs for the same inputs.  For example, for a male
  of age 10, according to the models, the height is variously $f(10) =
  50$ or $g(\mbox{male}) = 71$ or $h(10, \mbox{male}) = 51$.
  
\item  Many useful models don't give exactly the right output for
  every case.  Not every 10-year old male has the same height.  The
  difference between what a model says, and what the value is for an
  actual case, will not in general be zero.  The ``residual'' is the difference between
  the actual value for a given 10-year old male, say Ralph, and what
  a given model says about 10-year old males in general.  The
  residuals give important information about how good a model is.
   
\end{itemize}


\section{The Language of Models}
\label{sec:formula}

There is a language for defining models that is different from
writing down an algebraic formula.  

You have already seen some aspects of this notation in graphics
commands, e.g. \verb!height ~ sex!.  You can read this in any of several
ways:
\begin{itemize}
  \item Break down \VN{height} by \VN{sex} (as in a boxplot)
  \item \VN{height} versus \VN{sex}
  \item Model \VN{height} as a function of \VN{sex}.  
\end{itemize}
In this section, you'll see more complicated models, involving
multiple variables.  This makes it worthwhile to assign more precise
labels to the different components of the modeling language, which
has just a few components:
\begin{itemize}
  \item The response variable.  This is the output of the model function,
    \VN{height} in the above examples.
  \item The explanatory variables.  These are the inputs to the
    model function: \VN{age} and \VN{sex} in the above examples.
  \item Model terms.  Examples of model terms are explanatory
    variables themselves and a special term called the ``intercept.''
    There are a few others, but we'll deal with them as we come to them.
\end{itemize}
(A more comprehensive introduction is given in Chapters 4 and 5 of \cite{kaplan-2009-book}.)


As an example, consider this simple formula:
$$ z = 7 + 3 x + 4.5 y.$$
The corresponding model description consists of the response variable
$z$, and three model terms: the explanatory variables $x$ and
$y$ and the intercept term.  In the modeling language, it would be written:
\begin{quotation}
\centerline{\model{z}{1 + x + y}}
\end{quotation}
Notice that instead of the $=$ sign, we are using the $\sim$ sign.
Also, notice that the specific coefficients $7$, $3$, and $4.5$ are
missing.  The model description is a kind of skeleton for describing
the shape of the model.  It's like saying: ``We want a straight line
model,'' rather than giving the complete specification of the formula.

The process of finding a specific formula to make a model match the
pattern shown by data is called ``fitting the model to the data.''  

To see how different model specifications correspond to different
``shapes'' of models, consider the history of world-record times in the 100-meter 
free-style swimming race.

<<read-swim,echo=FALSE>>=
swim = read.csv("http://www.macalester.edu/~kaplan/ISM/datasets/swim100m.csv")
levels(swim$sex) <- c('Women','Men')
swim.plot = function(col='black',show.legend=TRUE,...){
plot( time ~ year, pch=19-4*as.numeric(sex), cex=1.0,xlim=c(1899,2010),
     cex.lab=1.0, cex.axis=1.0,data=swim,xlab='Year',
     ylab='Time (secs)',col=col,...)
  if( show.legend) legend(1980,92, legend=c("Women","Men"),pch=c(17,15),cex=1.0)
}
swim.plot2 = function(show.legend=TRUE,...){
print(
xyplot( time ~ year, data=swim, groups=sex, 
	 xlim=c(1899,2010), 
	 xlab='Year',
     ylab='Time (secs)',
	 auto.key= if (show.legend) list(columns=2) else FALSE,
	 ...,
	 par.settings = list( superpose.symbol=list(pch=c(17,15)) )
	 )
  )
}
@ 


<<swim-data-raw,echo=FALSE,eval=FALSE>>=
swim.plot( )
@ 

<<swim-data-raw2,echo=FALSE>>=
swim.plot2( )
@ 



You can see the steady improvement in records over the decades from
1900 to the present.  Men's times are somewhat faster than women's.

Now to build some models.

\subsection{\model{\VN{Time}}{\VN{1} + \VN{Year}}}

The model \model{\VN{time}}{1 + \VN{year}} gives the familiar
straight-line form: the intercept term (written simply as 1) and a term corresponding to the
linear dependence on \VN{year}.

<<swim-data-1,fig.keep=none,echo=FALSE>>=
swim.mod.plot = function(form,dots=FALSE, lwd=4) {
  t = as.character(form)
  t = paste(t[c(2,1,3)],collapse=" ")
  foo = lm( form, dat=swim)
  swim.plot(col='gray', show.legend=FALSE,
            main=t)
  if (dots)
      points( foo$fitted ~ swim$year, pch=20, cex=1.7,col='black')
  else 
      abline(foo$coef, lwd=lwd)
}
swim.mod.plot(time ~ year)
@ 


<<swim-data-1b,fig.show=hold,echo=FALSE>>=
swim.mod.plot2 = function(form, dots=FALSE, lwd=3, show.legend=FALSE,...){
  t = as.character(form)
  t = paste(t[c(2,1,3)],collapse=" ")
  model = lm( form, dat=swim )
  print(
  xyplot( time ~ year, data=swim, groups=sex, 
	 xlim=c(1899,2010), 
	 xlab='Year',
     ylab='Time (secs)',
	 main=t,
	 auto.key= if (show.legend) list(columns=2) else FALSE,
	 panel = function(x, y, ...) {
	 	panel.xyplot(x, y, col='gray', ...)
		if (dots) 
		  panel.points(x, fitted(model), cex=0.9, alpha=.8)
		else 
		  panel.abline(reg=model, lwd=lwd)
	 },
	 par.settings = list( superpose.symbol=list(pch=c(17,15)) ),
	 ...)
  )
}
swim.mod.plot2(time ~ year)
@ 

This model captures some of the pattern evident in the data: that
swimming times are improving (getting shorter) over the years.  And it
ignores other obvious features, for example the difference between men's
and women's times, or the curvature reflecting that records are not
improving as fast as they did in the early days.


\subsection{\model{\VN{Time}}{\VN{Sex}}}

The model \model{\VN{time}}{\VN{sex}} breaks down the swimming times
according to \VN{sex}:

<<swim-data-2,fig.show=hold,echo=FALSE>>=
swim.mod.plot2(time ~ sex, dots=TRUE)
@ 

This model reflects the typical difference between men's and women's
times.  It's oblivious to the trend that records improve over the
years.  Why?  Because the variable \VN{year} was not included in the model.

\subsection{\model{\VN{Time}}{\VN{Sex}+\VN{Year}}}

The record time evidently depends both on \VN{sex} and \VN{year}, so
it's sensible to include both variables in the model

<<swim-data-3,fig.show=hold,echo=FALSE>>=
swim.mod.plot2(time ~ sex + year, dots=TRUE)
@ 

\authNote{DTK: Should these graphics be re-written to display as
  continuous forms rather than dots?}

This is a straight-line model with separate lines for the different
sexes. The intercept is different for the different sexes, but the
slope is the same.  
This model is reflects the typical difference between men's and women's
times.  

Students sometimes observe that the function generated by fitting this
model doesn't respect the ``vertical line test'' taught in high-school
algebra.  This is a good time to remind students that this is a
function of \emph{two} variables.  It is indeed a function, and for
any specific value of the inputs \VN{sex} and \VN{year} gives a single value.

\subsection{\model{\VN{Time}}{\VN{Sex}+\VN{Year} + \VN{Sex}:\VN{Year}}}
 
There are two ways that the previous model, \model{\VN{time}}{\VN{sex}+\VN{year}},  misses obvious features in the
data: there is no curvature over the years and the slopes are exactly
the same for men and women.  To construct a model with different
slopes for men and women requires that we add a term that combines
both \VN{sex} and \VN{year}.  Such a term is constructed with the syntax
\VN{sex}:\VN{year} (or, what would amount to the same thing,
\VN{year}:\VN{sex}).  

<<swim-data-4,fig.show=hold,echo=FALSE>>=
swim.mod.plot2(time ~ sex * year, dots=TRUE)
@ 

In statistics, such a term is called an \emph{interaction term}.  (In
mathematics, it's often called a \emph{bi-linear term}.)  It's the
term that lets you have different slopes for the different sexes.  

The new phrase ``interaction term'' creates a need for a retronym, a way
to refer to those simple, non-interaction terms that we started with,
like \VN{sex} and \VN{year}.  (Common retronyms in everyday life are acoustic guitar, snail-mail, 
World War I, cloth diaper, and whole milk, compound terms that weren't needed
until electric guitars, e-mail, disposable diapers, and skim milk were
introduced, and World War II showed that the ``War to End All Wars''
was mis-named.)

The standard terminology for terms like \VN{sex} and \VN{year} is unfortunate: ``main effect.'' 
It suggests that interaction terms play a lesser role in modeling.  This is a bad
attitude, since sometimes the interaction is exactly what you're
interested in, but the terminology seems enshrined by statistical tradition.

Very often when you are including an interaction term, you want to
include the main effects as well.  There is a convenient shorthand for
this: \model{\VN{time}}{\VN{sex}*\VN{year}}

\subsection{The Intercept Only: \model{\VN{Time}}{\VN{1}}}

It's also possible to have models that have no explanatory variables
whatsoever.  Just the intercept term appears to the right of the
model formula. 

<<swim-data-5,fig.show=hold,echo=FALSE>>=
swim.mod.plot2(time ~ 1, dots=TRUE)
@ 

As you might expect, by leaving out both \VN{sex} and \VN{year} from
the model, it doesn't reflect the role of either variable in any way.
But the model \model{\VN{time}}{1} does get one thing very well: the
typical record time.  

Think of \model{\VN{time}}{1} as saying ``all the cases are the
same.''  In some ways, it's analogous to the model
\model{\VN{time}}{\VN{sex}}.  That model says that ``all men are the
same, and all women are the same.''  So the difference between 
\model{\VN{time}}{1} and \model{\VN{time}}{\VN{sex}} is just like the
difference between a ``grand mean'' and a ``group mean.''

\subsection{Transformation Terms}

You can construct more complicated models by adding in more
explanatory variables. (Improved swimming gear?  Better training?
Refinements in technique?).  You can also add in additional terms with
more structure.  There is a rich variety of ways to do this.

Since many students are familiar (or at least remember vaguely) the
idea of quadratics and polynomials, they might be interested to see
that the modeling language can handle this.  Here are three different
models involving a polynomial dependence on \VN{year}:

<<swim-data-6,fig.show=hold,echo=FALSE>>=
swim.mod.plot2(time ~ poly(year,2), dots=TRUE)
@ 

<<swim-data-7,fig.show=hold,echo=FALSE>>=
swim.mod.plot2(time ~ sex + poly(year,2), dots=TRUE)
@ 

<<swim-data-8,fig.show=hold,echo=FALSE>>=
swim.mod.plot2(time ~ sex * poly(year,2), dots=TRUE)
@ 

Although these models reflect more ``detail'' in the data, namely the
curvature, they do it in a way that ultimately does not make sense in
terms of the ``physics'' of world records.
Notice how the upward facing parabolas eventually produce a pattern
where the record times increase over the years.  

There are other sorts of nonlinear terms that might be more
appropriate for modeling this sort of data.  Exponentials, square
roots, etc., even piecewise linear or bent-line forms are all
possible within the modeling framework.  


\begin{problem}
{\bf DRAFT Outline} for a swim-data modeling problem: construct a post-war
variable and add it in.  Several ways to do this: explore which one's give
models that are most satisfactory to you:
\begin{itemize}
  \item postwar = year $>$ 1945
  \item interaction with postwar and year.
  \item pmax(year - 1945, 0)
\end{itemize}
\end{problem}





\section{Finding Formulas from Data}

Behind the graphical depictions of the models shown in the previous
section is a process for finding specific numerical formulas for the
models.  The software to do this is packaged into the \texttt{lm( )}
operator and is very easy for students and professionals alike.  The
human work, and what students need to learn, is not the mechanics of
fitting models, but the interpretation of them.  A good place to start
is with the interpretation of model coefficients.

To illustrate, consider the actual swimming records data employed in
the previous examples.  This data set is available via the internet,
and can be loaded into R with a command like the following:

\authNote{DTK: Eventually, we have to build into MOSAIC an easy way
  for instructors to specify a web location for their data.  The
  \texttt{ISMdata( )} function already does this in a limited way.}

<<swim-coeffs1>>=
swim = read.csv("http://www.macalester.edu/~kaplan/ISM/datasets/swim100m.csv")
@ 

\authNote{DTK: I couldn't stand the default printing of lm, so I
  changed it. We might want to do this more generally, but it's not
  something we need to worry about here.}

<<redefine-print-lm,echo=FALSE>>=
print.lm = function (x, digits = max(3, getOption("digits") - 3), ...) 
{
#    cat("\nCall:\n", paste(deparse(x$call), sep = "\n", collapse = "\n"), 
#        "\n\n", sep = "")
    if (length(coef(x))) {
        cat("Coefficients:\n")
        print.default(format(coef(x), digits = digits), print.gap = 2, 
            quote = FALSE)
    }
    else cat("No coefficients\n")
    cat("\n")
    invisible(x)
}
@ 
As before, we'll model the world-record \VN{time} as a function of \VN{sex}
and \VN{year}.

Here's the very simplest model: all cases are modeled as being the same.
<<swim-coeffs2>>=
lm( time ~ 1, data=swim )
@ 
The fundamental purpose of \texttt{lm( )} is to find the coefficients
that flesh out the skeleton provided by the model description.  For
this simple model, the coefficient works out to be the mean of the
record times:
<<>>=
with( data=swim, mean(time) )
@ 
\authNote{DTK: I'm looking forward to being able to write mean(time, data=swim).}

Models are more interesting that contain actual explanatory
variables.  Here's one using the quantitative variable \VN{year}:
<<swim-coeffs3>>=
lm( time ~ year, data=swim )
@ 
The coefficients now are the slope and intercept of the straight-line
relationship: $\mbox{time}  = -0.2599 \mbox{year} + 567.242$.

There's a similar story with categorical variables, like \VN{sex}:
<<swim-coeffs4>>=
lm( time ~ sex, data=swim )
@ 
Based on the result of the simple all-cases-the-same model
\model{\VN{time}}{1}, you might suspect that the coefficients are the
group means.  That's close to being right.  Here are the group means:
<<>>=
mean( time ~ sex, data=swim )
@ 
\InstructorNote{In the MOSAIC package, the syntax of familiar
  operators like \texttt{mean( )}, \texttt{sd( )}, etc. has been
  extended so that the modeling notation can be used to calculate
  group-by-group values.  Our goal is to make is straightforward to
  transition from conventional basic stats to modeling, by getting
  students used to the $\sim$ and \texttt{data=} notation early.  You
  can even do \texttt{mean( time}$ ~ $ \texttt{ 1, data=swim )}. 
  So you can talk about models in a systematic way even if all you want to cover is means,
  proportions, medians, etc.}


The coefficients of the linear model are organized differently than
simple group means.
Notice that there is a \VN{sexM} coefficient, but no similarly named
coefficient for women.  Instead, there is the intercept coefficient.  
This corresponds to the mean \VN{time} of one
of the groups: the reference group.  In this case, the reference group
is women.  

The other coefficient, \VN{sexM} tells the
\emph{difference} between the mean of the reference group and the mean
for the men.  In other words, the coefficients are arranged in
intercept-slope form, but for categorical variables the coefficient
isn't a slope but a finite-difference.  

\InstructorNote{The intercept is so important that the \texttt{lm( )}
  operator includes it by default, even if you don't specify it
  explicitly in the model design.  In those rare cases when you don't
  want an intercept term, use the notation \texttt{-1} as part of the
  model design.}


It's important to keep in mind the intercept-slope/difference format
when interpreting the coefficients from models with multiple
explanatory variables:
<<>>=
lm( time ~ sex + year, data=swim )
@ 
As usual, there is an intercept coefficient.  It's meaning is a little bit
subtle: it is the intercept for the reference group (in these data, women).  The coefficient
on \VN{year} is a slope.  The \VN{sexM} coefficient is a difference:
it's how the intercept differs for group \VN{M} from the reference
group.  

In traditional mathematical notation, the model formula corresponding
to these coefficients is
$$
\mbox{time} = - 0.2515 \ \mbox{year}  + \ \left\{ \begin{array}{rll}
  555.7168  &  & \mbox{for women}\\
555.7168  &  - 9.7980  & \mbox{for men}
\end{array}\right.
$$

\authNote{DTK: For problems, pull out some of the exercises from the
  coefficients chapter, including one on interaction terms. Use Question 1 from the 2011 ISM final
  exam.}

One of the great strengths of R comes from the ability to carry out
new computations on the results from other computations.  To
illustrate, give a name to a fitted model.
<<>>=
mod1 = lm( time ~ year + sex + year:sex, data=swim )
@ 

From the model, you can now compute additional information.  Important
operators for demonstrating basic properties of models are
\texttt{resid( )},  \texttt{fitted( )}, and \texttt{predict}.  


<<>>=
mean( resid(mod1) )
sd( resid(mod1) )
var( fitted(mod1) )/var(swim$time) # R-squared
sum( fitted(mod1)^2 ) # sum of squares of the fitted
sum( resid(mod1)^2 ) # sum of squares of the residuals
@ 

For plotting the fitted model values
<<eval=FALSE>>=
xyplot( fitted(mod1) ~ year, data=swim )
@ 

<<swim-plot-mod7,fig.show=hold,echo=FALSE>>=
print(xyplot( fitted(mod1) ~ year, data=swim ))
@ 

The model suggests that women's times will soon break those of men.
To evaluate the model for new values of inputs, use \texttt{predict( )}.
<<>>=
predict( mod1, newdata=data.frame(year=2020, sex="F") )
predict( mod1, newdata=data.frame(year=2020, sex="M") )
@ 


When you get to the stage where you want to talk about statistical
inference, you can of course show bootstrapping and permutation
tests.  For example for bootstrapping standard errors:
<<>>=
s = do(100)* lm( time ~ year + sex + year:sex, data=resample(swim) )
sd(s)
@ 

And, of course, you can do the conventional theoretical calculations
for inference.  
\begin{itemize}
\item Confidence intervals, for instance at a 95\% level
<<>>=
confint(mod1, level=0.95)
@ 
\item Analysis of variance
<<>>=
anova(mod1)
@ 
\item The regression report and other statistics on the model
<<>>=
summary(mod1)
@ 

\end{itemize}


\section{Example: Genetics before Genes}

To emphasize the flexibility that multivariate models gives in asking
questions of statistical interest, let's consider a problem of
historical significance: Francis Galton's attempt to quantify the heritability
of height.

Galton famously developed the correlation coefficient in the late
1880s (see \cite{galton-co-relations}.)  One of his motivations was to
put on a quantitative footing the theory of evolution established
by his half-cousin, Charles Darwin.  It's important to realize that
Darwin himself did not know the mechanism by which traits were
inherited.  He imagined a particle of inheritance which he called a
``gemmule.''  The words ``gene'' and ``genetics'' date from the first
decade of the 20th century, the same decade when William Gossett started
publishing under the pseudonym ``Student.''  It wasn't until 1944 when
DNA was shown to be associated with genetic heritability.

Without a mechanism of genotype, Galton needed to rely on what we now
call ``phenotype,'' the observable characteristics themselves.  The 
\pkg{mosaic} package
includes the 
\texttt{Galton} dataset, transcribed to a modern format, of measurements that Galton himself
collected on the heights of adult children and their parents. 



<<galton-data>>=
data(Galton)
head(Galton)
@ 

By today's standards, Galton's techniques were quite limited, but did
suffice to quantify in some ways the heritability of height.  Galton
examined, for the boys, the correlation between the height of the
``mid-parent'' and the boy's height.  If Galton had R available (rather than
having to invent $r$!), he might have done the following calculation:
<<>>=
Galton$midparent = (Galton$father + 1.08*Galton$mother)/2
boys = subset(Galton, sex=="M")
with( boys, cor(midparent,height))
@ 

This is purely speculation, but it seems unlikely that Galton,
with his interests ranging from exploring Namibia to fingerprints to meteorology (and, it
must be mentioned, eugenics), would have restricted himself to a
correlation between the mid-parent and boy's heights.  Might height be
inherited differently from the mother and the father?  Is the same
mechanism at work for girls as for boys?  Does one parent's height
have a potentiating effect on the influence of the other parent's height?

Presumably, Galton would have constructed more detailed descriptions
of the relationship between a child's adult height and the genetic and
environmental influences, rather than focusing on only the parent's
height.  Even with the few variables in Galton's own dataset, one can
use models to explore hypothesized relationships.

By all means, use Galton's data to illustrate simple modeling
techniques, e.g., 
<<>>=
lm( height ~ father + sex, data=Galton )
@ 

But consider going further and using inferential methods to see what
evidence is contained in Galton's data more detailed descriptions.
For instance, in the relatively simple model
<<>>=
summary( lm( height ~ sex + father + mother + nkids, data=Galton) )
@ 

Ask your students to investigate, through modeling, whether the
influence of the parents is different for the different sexes, or
whether what looks like an even split of influence between the father
and mother could all, in fact, be attributed to the mother.

\begin{problem}
%[stars=1]
Consider the \texttt{CPS} data that gives per-hour wages and
  other measurements for workers in the mid-1980s.  Is there evidence in the data for
  discrimination on the basis of race and/or sex?  What covariates
  might one reasonably adjust for in measuring the difference between
  the races or the sexes?
\end{problem}
  

\shipoutProblems



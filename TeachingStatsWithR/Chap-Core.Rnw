

\chapter{The Core of a Traditional Course}


\SweaveOpts{
tidy=TRUE,
dev=pdf,
fig.path=figures/fig-,  
fig.width=5, fig.height=2.2,
fig.align=center,
fig.show=hold,
comment=NA
}
<<setup,echo=FALSE,message=FALSE,results=hide>>=
source('setup.R')
@ 

In this chapter, we will briefly review the commands and functions needed
to analyze data from a more traditional introductory statistics course.
We will use data from the HELP study: a randomized trial of a novel 
way to link at-risk subjects with primary care.  More information on the
dataset can be found in section \ref{sec:help}.

Some data management is needed by students (and more by instructors).  This
material is reviewed in section \ref{sec:manipulatingData}.

Since the selection and order of topics can vary greatly from 
textbook to textbook and instructor to instructor, we have chosen to 
organize this material by the kind of data being analyzed.  This should make
it straightforward to find what you are looking for even if you present 
things in a different order.  This is also a good organizational template
to give your students to help them keep straight ``what to do when".

\InstructorNote{Just what is the \emph{traditional course}, anyways?}
It's worth noting how we teach the ``Traditional Course'', which, contrary
to rumor, some of us do on occasion.  Nick has used the 5th edition of
Moore and McCabe \cite{moor:mcca:2006}, where he introduces least squares 
regression at the end of the first week of a semester, and multiple regression
(purely descriptively) at the end of the second week.  Design is the next
main topic, then a brutally pruned intro to probability and sampling distributions.
Interval estimation and testing are introduced in one or two settings, then 
the class returns to inference for multiple regression.  While students are working
on projects involving a multiple linear regression model with 2 predictors,
some categorical data analysis is covered.

\authNote{Danny and/or Randy: interested in chiming in?}

\section{One Quantitative Variable}

\subsection{Numerical Summaries}

\R\ includes a number of commands to numerically summarize variables.
These include the capability of calculating the mean, standard deviation,
variance, median, five number summary, intra-quartile range (IQR) as well as arbitrary quantiles.  We will
illustrate these using the CESD (Center for Epidemiologic Studies - Depression)
measure of depressive symptoms.  To improve the legibility of output,
we will also set the default number of digits to display to a more reasonable
level (see {\tt ?options} for more configuration possibilities).

<<cesd1,cache=FALSE>>=
options(digits=3)
mean(HELP$cesd)
@
<<cesd2,cache=FALSE>>=
sd(HELP$cesd)
@
<<cesd2b,cache=FALSE>>=
sd(HELP$cesd)^2; var(HELP$cesd)
@

It is also straightforward to calculate quantiles of the distribution.

<<cesd3>>=
median(HELP$cesd)
five <- fivenum(HELP$cesd)
five     # display the object (vector of length 5)
@
The user can interact with the objects created by these
functions to calculate the IQR (or the built-in a function can generate this directly).
\Caution{Note that the hinges are not necessarily the same as the
quartiles, due to interpolation in small datasets}
<<>>=
five[4] - five[2]
IQR(HELP$cesd)
@


<<cesd4>>=
quantile(HELP$cesd)   
quantile(HELP$cesd, c(.25, .50))
favstats(HELP$cesd)
@

By default, the 
\function{quantile()} function displays the quartiles, but can be given
a vector of quantiles to display.  Finally, the \function{favstats()}
function in the \pkg{mosaic} package provides a concise summary of 
many useful statistics.


\subsection{Graphical Summaries}
The \function{histogram()} function is used to create a histogram.
Here we use the formula interface (as discussed in section \ref{sec:formula}) to
specify that we want a histogram of the CESD scores.

\vspace{-4mm}
\begin{center}
<<cesd-hist,fig.show=hold>>=
histogram(~ cesd, data=HELP)
@
\end{center}


In the HELP dataset, approximately one quarter of the subjects are female.  
<<>>=
with( HELP, table(sex) )
@
It is straightforward to restrict our attention to just those subjects.
If we are going to do many things with a subset of our data, it may be easiest
to make a new data frame containing only the cases we are interested in.
The \function{subset()} function can generate a new data frame containing
just the women or just the men  (see also section \ref{sec:subsets}).  Once this is created, we
used the \function{stem()} function to create a stem and leaf plot.
\Caution{Note that the equality operator is \emph{two} equal signs}
<<cesd-stem>>=
female <- subset(HELP, sex=='female')
male <- subset(HELP, sex=='male')
stem(female$cesd)
@

The plotting functions in the \pkg{lattice} package have a \option{subset}
argument that allows us to select only a portion of the data.
This can be handy for explaration and avoids the need to make (and name)
various subsets.
<<women-cesd-hist,fig.show=hold>>=
histogram(~ cesd, data=HELP, subset=sex=='female')   # notice the quotes
@

Alternatively, we can make side-by-side plots to compare multiple subsets.
<<cesd-male-female,fig.show=hold>>=
histogram(~ cesd | sex, data=HELP)
@

\begin{problem}
Using the HELP dataset, 
create side by side boxplots of the CESD scores by substance abuse
group, just for the male subjects.
\end{problem}
\begin{solution}
<<subsmale,fig.show=hold>>=
bwplot(cesd ~ substance, data=HELP, subset=sex=='male')
@
\end{solution}

The \function{dotPlot()} function is used to create a dotplot (a la Fathom).

\begin{center}
<<cesd-dot,fig.show=hold,fig.height=1.8>>=
histogram(~ cesd, nint=20, data=female)   # was dotPlot
@
\end{center}

We could also have made our subset ``on the fly'', just for the purposes of graphing:
\begin{center}
<<cesd-dot2,fig.show=hold,fig.height=1.8>>=
histogram(~ cesd, data=HELP, subset=(sex=='female'))   # was dotPlot
@
\end{center}

Or we could make side-by-side dotplots of men and women:
\begin{center}
<<cesd-dot3,fig.show=hold>>=
histogram(~ cesd | sex, data=HELP)   # was dotPlot
@
\end{center}


\subsection{Density Curves}

One disadvantage of histograms is that they can be sensitive to the choice of the
number of bins.  Another display to consider is a density curve.
\FoodForThought{Density plots are also sensitive to certain choices.  If your density plot
is too jagged or too smooth, try adjusting the \option{adjust} argument (larger than 1 for
smoother plots, less than 1 for more jagged plots.}

\begin{center}
<<dens1,fig.show=hold>>=
densityplot(~ cesd, data=female)
ladd(grid.text(x=0.2, y=0.8, 'only females'))
ladd( panel.mathdensity(args=list(mean=mean(female$cesd), sd=sd(female$cesd)),
  col="red"))
@
\end{center}

The most famous density curve is a normal distribution.  The \function{xpnorm()} function
displays the probability that a random variable is less than the first argument, for a 
normal distribution with mean given by the second argument and standard deviation by the 
third.  More information about probability distributions can 
be found in section \ref{sec:probability}.

\subsection{Normal Distributions}
\begin{center}
<<norm1,fig.show=hold,fig.width=5,fig.height=2.4>>=
xpnorm(1.96, 0, 1)
@
\end{center}

\subsection{Inference for a single sample}

We can calculate a 95\% confindence interval for the mean CESD 
score for females by using a t-test:
<<tinterval>>=
t.test(female$cesd)
@

But it's also straightforward to calculate this using a bootstrap.
The statistic that we want to resample is the mean.  
<<calculuatemean>>=
with( female, mean(cesd) )
@

One resampling trial can be carried out with
<<resamplemean>>=
with( resample(female), mean(cesd) )
@
Even though a single trial is of little use, it's a nice idea to have
students do the calculation to show that they are getting a different
(usually!) result than without resampling.

Now conduct 1000 resampling trials, saving the results in an object
called \texttt{trials}:
<<bootint>>=
trials = do(1000) *
  with( resample(female), mean(cesd) )
quantile(trials$mean, c(.025, .975))
@

\section{One Categorical Variable}

\subsection{Numerical Summaries}

The \function{xtabs()}, \function{perctable()} and \function{proptable()} functions can be used to calculate
counts, percentages and proportions, respectively for a categorical variable.

<<homeless-table>>=
xtabs(~ homeless, HELP)      # display table using counts
perctable(HELP$homeless)     # display table using percentages
proptable(HELP$homeless)     # display table using proportions
@
\authNote{NH: how best to add row and/or column totals?  rjp:  Seems like someone must
have done this already, but I don't know of a function to do it.  I've thought about writing one.  NH: would "addmargins()" be helpful?}

\subsection{The Binomial Test}

An exact confidence interval for a proportion (as well as a test of the null 
hypothesis is equal to a particular value [by default 0.5]) can be calculated
using the \function{binom.test()} function.
The standard \function{binom.test()} requires us to tabulate.
<<>>=
binom.test(209, 209 + 244)
@
The \pkg{mosaic} package provides a formula interface that avoids the need to pre-tally
the data.
<<binomtest>>=
result <- binom.test(~ homeless=="homeless", HELP)
result
names(result)
@

As is generally the case with commands of this sort, 
there are a number of useful quantities available from 
the object returned by the function.  
For example, the user can extract the confidence interval either manually
<<binomtest2>>=
result$conf.int
result$conf.int[1]
@
or via the \function{interval()} function in the \pkg{mosaic} package.%
\footnote{A word of explanation is in order here.  In \R\ what you see is not 
what you get.  Most of the objects we encounter in \R\ have a \function{print()}
method.  So when we get \code{result}, what we are seeing displayed is
\code{print(result)}.  There may be a good deal of additional information
lurking inside the object itself.  To make matter even more complicated, some
objects are returned \emph{invisibly}, so nothing prints.  You can still assign
the returned object to a variable and process it later, even if nothing shows up
on the screen.  This is the case for the \pkg{lattice} graphics functions, for example.
You can save a plot into a variable, say \code{myplot} and display the plot again later 
using \code{print(myplot)}.}%
\InstructorNote{You may want to read the footnote that describes what's going on here in 
a bit more detail.}%
<<>>=
interval(result)
@
We can extract the p-value similarly.
<<>>=
pval(result)
@


\subsection{The Proportion Test}

A similar interval and test can be calculated using \function{prop.test()}.
This takes a table (as generated by \function{xtabs()} as an argument).  
<<>>=
prop.test(xtabs(~ homeless=="housed", HELP), correct=FALSE)
@
It also accepts summarized data, the way \function{binom.test()} does.
\InstructorNote{\function{prop.test()} uses a Chi-squared statistic.
Most introductory texts use a $z$-statistic.  They are equivalent, but
you may need to address this with your students.}%
<<>>=
prop.test(209, 209 + 244, correct=FALSE)
@
or counts of successes and failures as a single vector.
<<>>=
prop.test(c(209, 244), correct=FALSE)
@
To make things simpler still, we've added a formula interface in the \pkg{mosaic} package.
<<>>=
prop.test(~ homeless, data=HELP)
@

\subsection{Goodness of Fit Tests}

A variety of goodness of fit tests can be calculated against a reference 
distribution.  For the HELP data, we could test the null hypothesis that the 
proportion of subjects back in the original population in each of the 
substance abuse groups were equal.

<<gof>>=
substab <- xtabs(~ substance, data=HELP)
substab
@
<<gofmore>>=
perctable(HELP$substance)
p <- c(1/3, 1/3, 1/3)   # equivalent to rep(1/3, 3)
chisq.test(substab, p=p)
total <- sum(substab); total
total*p
@

We can also calculate this quantity manually, in terms of observed and expected values.

\InstructorNote{We don't encourage much manual calculations in our courses}
<<gof2>>=
chisq <- sum((substab - total*p)^2/(total*p)); chisq
1 - pchisq(chisq, 2)
@

Alternatively, the \pkg{mosaic} package provides a version of \function{chisq.test()} with
more verbose output.
\FoodForThought{\code{x} is for eXtra.}
<<>>=
xchisq.test(substab, p=p)
rm(substab, p, total, chisq)                       # disposing of variables no longer needed.
@


\section{Two Quantitative Variables}

\subsection{Scatterplots}

We always encourage students to start any analysis by graphing their data.  
Here we augment a scatterplot
of the CESD (depressive symptoms) and the MCS (mental component score from the SF-36)
with a lowess (locally weighted scatterplot smoother) line.  

\InstructorNote{The lowess line can help to assess linearity of a relationship}
\begin{center}
<<HELP-xyplot,fig.show=hold,fig.height=3>>=
xyplot(cesd ~ mcs, data=HELP, type=c('p','smooth'), pch=1)
@
\end{center}

For multivariate data sets, \function{splom()} will generate pairwise
scatterplots for selected variables.
<<HELP-splom,fig.show=hold,fig.height=4>>=
splom(HELP[,c("age","cesd","d1")])
@


\subsection{Correlation}

Correlations can be calculated for a pair of variables, or for a matrix of variables.
<<>>=
with(HELP, cor(cesd, mcs))
with(HELP, cor(cbind(cesd, mcs, pcs)))
@

By default, Pearson correlations are provided, other variants can be specified using the
\option{method} option.
<<>>=
with(HELP, cor(cesd, mcs), method="spearman")
@

\subsection{Simple Linear Regression}

\InstructorNote{As mentioned earlier, we tend to introduce linear regression
early in our courses.}

Linear regression models were introduced previously in 
section \ref{sec:lm}.  These use the same formula interface to specify the outcome 
and predictors.  Here we consider fitting the model \model{\VN{cesd}}{\VN{mcs}}.


<<>>=
model <- lm(cesd ~ mcs, data=HELP)
coef(model)
@
<<>>=
summary(model)
@


<<lmclass>>=
class(model)
@
The return value from \function{lm()} is a linear model object;
A number of functions can operate on these objects, as
seen previously with \function{coef()}.  As an additional example,
there is a function called \function{residuals()} that will return a
vector of the residuals.
\FoodForThought{\function{residuals()} can be abbreviated 
\function{resid()}.}
\InstructorNote{Another useful function is \function{predict()} }

\begin{center}
<<lmhist,fig.show=hold>>=
xhistogram(~ residuals(model), density=TRUE)
@
\end{center}
\begin{center}
<<HELP-resid-qq,fig.show=hold>>=
qqmath(~ resid(model))
@
\end{center}
\begin{center}
<<HELP-resid-plot,fig.show=hold>>=
xyplot(resid(model) ~ fitted(model))
@
\end{center}


In addition, the \function{plot()} method can produce a number of diagnostic plots
(see \texttt{?plot.lm}).

\begin{multicols}{2}
\begin{center}
<<HELP-lmplot1,fig.show=hold,fig.width=3,fig.height=4>>=
plot(model, which=1)
@
<<HELP-lmplot2,fig.show=hold,fig.width=3,fig.height=4>>=
plot(model, which=2)
@
\end{center}
\end{multicols}

\begin{problem}
Using the HELP dataset, fit a simple linear regression model 
predicting the number of drinks per day as a function of the mental
component score.  

This model can be specified using the formula:
\model{\VN{i1}}{\VN{mcs}}.  

Assess the distribution of the residuals for this model.
\end{problem}


\section{Two Categorical Variables}


\subsection{Cross classification Tables}

Cross classification (two-way or $R$ by $C$) tables can be constructed for
two (or more) categorical variables.  Here we consider the contingency table
for homeless status (homeless one or more nights in the past 6 months or housed) 
and sex.

\subsubsection{From Raw Data}
<<homeless-sex>>=
xtabs(~ homeless + sex, data=HELP)     
@

\subsubsection{Building Cross Tables from Already Summarized Data}

\authNote{Need to decide how we want to do this.}

\subsubsection{Epidemiologic statistics}
\label{sec:epistats}

To demonstrate how to extend \R\ using your own functions, we can write
one which calculates epidemiologic statistics such as the odds ratio (OR).
More information about writing your own functions can be found in 
section
\ref{sec:writingFunctions}.

<<orfunc>>=
odds.ratio <- function(x, y, ds) {
  tab = xtabs(~ x + y)
  return(tab[1,1]*tab[2,2]/(tab[1,2]*tab[2,1]))
}
with(HELP, odds.ratio(homeless, sex))
@

\authNote{NH to expand}

\subsubsection{Graphical Summaries}

Graphical summaries of cross classification tables may be helpful in visualizing
associations.  Mosaic plots are one example (though the jury is still out
regarding its utility, relative to the low data to ink ratio \cite{Tufte:2001:Visual}.  
Here we see that males tend to be over-represented
amongst the homeless subjects (as represented by the horizontal line which is higher for
the homeless rather than the housed).  
\FoodForThought{The \function{mosaic()} function 
in the \pkg{vcd} package also makes mosaic plots.}

\begin{center}
<<mosaicplot,fig.show=hold,fig.height=3.5>>=
mosaicplot(xtabs(~ homeless + sex, data=HELP))
@
\end{center}

\subsection{Chi-squared tests}

%with(HELP, chisq.test(homeless, sex, correct=FALSE))
<<chisq1>>=
chisq.test(xtabs(~ homeless + sex, data=HELP), correct=FALSE)
@

There is a statistically significant association found: it is unlikely that we would observe
an association this strong if there was homeless status and sex were independent back in the 
population.

\subsubsection{Displaying additional information}

When a student finds a significant association, 
it's important for them to be able to interpret this in the context of the problem. 
The \function{xchisq.test()} function provides additional details to help with this process.

<<chisq2>>=
xchisq.test(xtabs(~homeless + sex, data=HELP), correct=FALSE)
@

We observe that there are fewer homeless women, and more homeless men that would be expected.

\subsection{Fisher's Exact Test}

An exact test can also be calculated.  This is fairly computationally straightforward for 2 by 2
tables.  Options to help constrain the size of the problem for larger tables exist
(see \verb!?fisher.test()!).

%with(HELP, fisher.test(homeless, sex))
<<help-fisher>>=
fisher.test(xtabs(~homeless + sex, data=HELP))
@

\section{Quantitative Response to a Categorical Predictor}

\subsection{A Bivariate Predictor: Numerical and Graphical Summaries}
Here we will compare the distributions of CESD scores by sex.

The \function{aggregate()} function can be used to calculate the mean CESD score
for each of the two groups 
<<aggregate>>=
aggregate(cesd ~ sex, data=HELP, FUN=mean)
@

The \function{summary()} function can provide more statistics by group.
<<summary>>=
summary(cesd ~ sex, data=HELP, fun=favstats)
@


Boxplots are a particularly helpful graphical display to compare distributions.
The \function{bwplot()} function can be used to display the boxplots for the
CESD scores separately by sex.  We see from both the numerical and graphical
displays that women tend to have slightly higher CESD scores than men.

\FoodForThought{Although we usually put explanatory variables along the horizontal axis,
page layout sometimes makes the other orientation preferable for these plots.}
%\vspace{-8mm}
\begin{center}
<<cesd-box,fig.show=hold,fig.height=1.5>>=
bwplot(sex ~ cesd, data=HELP)
@
\end{center}

When sample sizes are small, there is no reason to summarize with a boxplot
since  \function{xyplot()} can handle categorical predictors.
Even with 10--20 observations in a group, a scatter plot is often quite readable.
Setting the alpha level helps detect multiple observations with the same value.
\begin{center}
<<KidsFeet-xy,fig.show=hold,fig.height=1.5>>=
xyplot(sex ~ length, KidsFeet, alpha=.6, cex=1.4) 
@
\end{center}
\FoodForThought{One of us once saw a biologist proudly present
side-by-side boxplots.  Thinking a major victory had been won, he naively
asked how many observations were in each group.  ``Four,'' replied the 
biologist.}

\subsection{A Bivariate Predictor: Two-sample t}

The Student's two sample t-test can be run without or with an equal variance assumption.

<<help-nonpar>>=
t.test(cesd ~ sex, data=HELP, var.equal=FALSE)
@

We see that there is a statistically significant difference between the two groups.


\subsection{Non-parametric 2 group tests}

The same conclusion is seen using a non-parametric (Wilcoxon rank sum) test.

<<help-nonpar>>=
wilcox.test(cesd ~ sex, data=HELP)
@


\subsection{Permutation test}

Here we extend the methods introduced in section \ref{sec:comparing-two-means} to 
undertake a two-sided test.

%rtest.stats[simulations+1,] <- test.stat  # add in the observed value
<<permute-HELP,fig.show=hold>>=
simulations <- 500
test.stat <-  abs(diff(aggregate(cesd ~ sex, data=HELP, mean)$cesd))
test.stat
rtest.stats <- do(simulations) * c(diff= diff(aggregate(cesd ~ shuffle(sex), HELP, mean)$cesd))
histogram(~ abs(diff), rtest.stats, n=40, xlim=c(0, 6),
  groups=abs(rtest.stats$diff) >= test.stat, pch=16, cex=.8 ) # was dotPlot
ladd(panel.abline(v=test.stat))
@


\authNote{NH: line still isn't showing up}

The same conclusion is observed with a permutation test (the
observed value is nowhere in the vicinity of the permuted values).

\subsection{One-way ANOVA}

Earlier comparisons were between two groups: we can also consider testing differences 
in CESD scores by primary substance of abuse (heroin, cocaine, or alcohol).

\begin{center}
<<cesd-oneway,fig.show=hold,fig.width=6,fig.height=1.9>>=
bwplot(cesd ~ substance, data=HELP)
@
\end{center}


<<aggregate2>>=
aggregate(cesd ~ substance, data=HELP, FUN=mean)
<<help-aov>>=
mod <- aov(cesd ~ substance, data=HELP)
summary(mod)
@
While still high (scores of 16 or more are generally considered to be 
``severe'' symptoms), the cocaine-involved group tend to have lower 
scores than those whose primary substances are alcohol or heroin.
<<help-aovlm>>=
mod1 <- lm(cesd ~ 1, data=HELP)
mod2 <- lm(cesd ~ substance, data=HELP)
@

The \function{anova()} command can be used to formally 
compare two (nested) models.

<<help-aov2>>=
anova(mod1, mod2)
@


\subsection{Tukey's Honest Significant Differences}

There are a variety of multiple comparison procedures that can be
used after fitting an ANOVA model.  One of these is Tukey's Honest
Significant Difference (HSD).  Other options are available with the 
\pkg{multcomp} package.

<<help-hsd>>=
with(HELP, tapply(cesd, substance, mean))
@
<<help-hsd2>>=
compare <- TukeyHSD(mod, "substance")
compare
@
<<help-hsd3,fig.show=hold,fig.height=4>>=
plot(compare)
@

Again, we see that the cocaine group has significantly lower CESD scores
than the other two groups.

\section{Categorical Response to a Quantitative Predictor}

\subsection{Logistic Regression}

Logistic regression is available from within the \function{glm()} function: a variety of
link functions and distributional forms for generalized linear models are supported.

\FoodForThought{\function{glm()} has arguments \option{family}  and
\option{link}.  The \code{logit} link is the default link for the binomial family,
so we don't need to specify it here.}
<<help-logit>>=
logitmod <- glm(homeless ~ age + female, family=binomial, data=HELP)
summary(logitmod)
exp(coef(logitmod))
@

\section{Survival Time Outcomes}

Extensive support for Survival (Time to Event) analysis is available within the 
\pkg{survival} package.

\subsection{Kaplan-Meier plot}

\begin{center}
<<help-km, fig.show=hold,fig.width=6,fig.height=3.9>>=
library(survival)
fit <- survfit(Surv(dayslink, linkstatus) ~ treat, data=HELP)
plot(fit, conf.int=FALSE, lty=1:2, lwd=2, xlab="time (in days)",
  ylab="P(not linked)")
legend(20, 0.4, legend=c("Control", "Treatment"), lty=c(1,2),
  lwd=2)
title("Product-Limit Survival Estimates (time to linkage)")
@
\end{center}

We see that the subjects in the treatment (HELP clinic) were significantly more likely to 
link to primary care than the control (usual care) group.

\subsection{Cox proportional hazards model}

<<help-surv>>=
library(survival)
summary(coxph(Surv(dayslink, linkstatus) ~ age + substance, data=HELP))
@

Neither age, nor substance group was significantly associated with linkage to primary care.


\section{More than Two Variables}

\subsection{Two (or more) way ANOVA}

We can fit a two (or more) way ANOVA model, without or with an interaction,
using the same modeling syntax.
<<help-aovplot,fig.show=hold>>=
bwplot(cesd ~ substance | sex, data=HELP)
@
<<help-aov2>>=
summary(aov(cesd ~ substance + sex, data=HELP))
@
<<help-aov3>>=
summary(aov(cesd ~ substance * sex, data=HELP))
@
There's little evidence for the interaction, though there are statistically
significant main effects terms for \VN{substance} group and 
\VN{sex}.

<<help-interaction,fig.show=hold>>=
xyplot(cesd ~ substance, data=HELP, groups=sex, type='a')
@


\subsection{Multiple Regression}

<<help-multreg>>=
lm1 <- lm(cesd ~ mcs + age + sex, data=HELP)
summary(lm1)
@
<<help-multreg2>>=
lm2 <- lm(cesd ~ mcs + age + sex + mcs*sex, data=HELP)
summary(lm2)
@
<<help-multreg3>>=
anova(lm1, lm2)
@
There is little evidence for an interaction effect: we may want
to drop this from the model.



\section{Probability and Random Variables}

\label{sec:DiscreteDistributions}
\label{sec:probability}

\R\ can calculate quantities related to probability distributions of all types.  
It is straightforward to generate
random variables from these distributions, which can be used 
for simulation and analysis.

Table \ref{core:dist} displays the basenames for probability distributions 
available within base \R.  These functions can be prefixed by {\tt d} to 
find the density function for the distribution, {\tt p} to find the 
cumulative density function, {\tt q} to find quantiles, and {\tt r} to 
generate random draws. For example, to find the density function of a binomial
random variable, use the command \function{dbinom()}.
The \function{qDIST()} function is the inverse of the 
{\tt pDIST()} function, for a given basename {\tt DIST}. 

<<probdist>>=
pnorm(1.96, 0, 1)    # P(Z < 1.96)
qnorm(.975, 0, 1)
integrate(dnorm, -Inf, 0)
@


\begin{table}
\begin{center}
\caption{basename for probability distribution and random number generation}
\label{core:dist}
\begin{tabular}{|c|c|c|} \hline
Distribution   & NAME                 \\ \hline
Beta           &  {\tt beta}        \\
binomial       &  {\tt binom}    \\
Cauchy         &  {\tt cauchy}   \\
chi-square     &  {\tt chisq}    \\
exponential    &  {\tt exp}      \\
F              &  {\tt f}        \\
gamma          &  {\tt gamma}    \\
geometric      &  {\tt geom}     \\
hypergeometric &  {\tt hyper}    \\
logistic       &  {\tt logis}    \\
lognormal      &  {\tt lnorm}    \\
negative binomial &  {\tt nbinom} \\
normal         &  {\tt norm}      \\
Poisson        &  {\tt pois}      \\
Student's t    &  {\tt t}        \\
Uniform        &  {\tt unif}     \\
Weibull        &  {\tt weibull}   \\ \hline
\end{tabular}
\end{center}
\end{table}

\begin{problem}
Generate a sample of 1000 exponential random variables with rate parameter
equal to 2, and calculate the mean of those variables.
\end{problem}
\begin{solution}
<<expprob>>=
x <- rexp(1000, 2)
mean(x)
@
\end{solution}

\begin{problem}
Find the median of the random variable X, if it is exponentially distributed
with rate parameter 10.
\end{problem}
\begin{solution}
<<expprob2>>=
qexp(.5, 10)
@
\end{solution}


\section{Power Calculations}

\label{sec:onesamppower}

While not generally a major topic in introductory courses, power and sample size calculations
help to reinforce key ideas in statistics.  In this section, we will explore how \R\ can 
be used to undertake power calculations using analytic approaches (see \ref{sec:power}
for simulation based approaches).  We consider a simple problem with two tests (t-test and
sign test) of
a one-sided comparison.

Let $X_1, ..., X_{25}$ be i.i.d. $N(0.3, 1)$.  Consider testing the null hypothesis $H_0: \mu=0$ versus $H_A: \mu>0$ at significance level $\alpha=.05$.  Compare the power of the sign test and the power of the test based on normal theory (one sample one sided t-test) assuming that $\sigma$ 
is known.

\subsection{Sign test}

We first start by calculating the Type I error rate for the sign test.  Here we want to
reject when the number of positive values is large.  Under the null hypothesis, this is
distributed as a Binomial random variable with n=25 trials and p=0.5 probability of being
a positive value.  Let's consider values between 15 and 19.
<<pbinom>>=
xvals <- 15:19
probs <- 1 - pbinom(xvals, 25, 0.5)
cbind(xvals, probs)
@
So we see that if we decide to reject when $X =$ the number of positive values is
17 or larger, we will have an $\alpha$ level of \Sexpr{round(1-pbinom(16, 25, 0.5), 3)},
which is near the nominal value in the problem.

We calculate the power of the sign test as follows. The probability that $X > 0$, given that $H_A$ is true is given by:
<<pnorm1>>=
1-pnorm(0, 0.3, 1)
@
We can view this graphically using the command:

\begin{center}
<<pnorm2,fig.show=hold,fig.width=5,fig.height=1.9>>= 
xpnorm(0, 0.3, 1)
@
\end{center}

So now we need to find the power under the alternative, which is equal to the probability of getting 17 or more positive values,
given that $p=0.6179$:
<<pbinom2>>=
1 - pbinom(16, 25, 0.6179)
@

The power is modest at best.

\subsection{T-test}

We next calculate the power of the test based on normal theory.  To keep the comparison
fair, we will set our $\alpha$ level equal to 0.05388.
First we need to find the rejection region.  

<<>>=
alpha <- 1-pbinom(16, 25, .5) # == 0.0539, our alpha level.
n <- 25; sigma <- 1 # given
stderr <- sqrt(sigma^2/n)
zstar <- qnorm(1-alpha, 0, 1)
zstar
crit <- zstar*stderr
crit
@
Therefore, we reject for observed means greater than \Sexpr{round(crit,3)}.  

To calculate the power of this one-sided test we find the area 
under the alternative hypothesis 
that is to the right of this cutoff:
<<>>=
power <- 1 - pnorm(crit, 0.3, stderr)
power
@
Thus, the power of the test based on normal theory is \Sexpr{round(power,3)}.
To provide a general check (or for future calculations of this sort) we can use the {\tt power.t.test()} in \R.
<<>>=
power.t.test(n=25, delta=.3, sd=1, sig.level=alpha, alternative="one.sided",
type="one.sample")$power
@

This yields a similar estimate to the value that we calculated directly.  
Overall, we see that the t-test has higher power than the sign test, if the underlying
data are truly normal.  It's useful to have students calculate power empirically, 
as demonstrated for this example in \ref{sec:power}.

\begin{problem}
\label{prob:power1}
<<setup,echo=FALSE>>=
n <- 100
alpha <- 0.01
@
Find the power of a two-sided two-sample t-test where both distributions 
are approximately normally distributed with the same standard deviation, but the group differ by 50\% of the standard deviation.  Assume that there are 
\Sexpr{n}
observations per group and an alpha level of \Sexpr{alpha}.

\end{problem}
\begin{solution}
<<power>>=
n
alpha
power.t.test(n=n, delta=.5, sd=1, sig.level=alpha)
@
\end{solution}

\begin{problem}
Repeat problem \ref{prob:power1} by simulating using the methods of section \ref{sec:power}.
\end{problem}

\begin{problem}
Find the sample size needed to have 90\% power for a two group t-test
where the true 
difference between means is 25\% of the standard deviation in the groups
(with $\alpha=0.05$).
\end{problem}
\begin{solution}
<<power2>>=
power.t.test(delta=.25, sd=1, sig.level=alpha, power=0.90)
@
\end{solution}


\section{Exercises and Problems}

\shipoutProblems




\chapter{Multivariate Statistics -- Early?}

\label{chap:multivariate-early}

\SweaveOpts{prefix.string=figures/multivar}  % location of 
\SweaveOpts{highlight=T}    % not sure this does anything unless we use pgfSweave
\SweaveOpts{tidy=F}         % keep.source probably disables this
\SweaveOpts{pdf=T}          % use pdf for graphics
\SweaveOpts{strip.white=T}  % remove blank lines at beginning and end 
\SweaveOpts{keep.source=T}  % keeps formatting from original; allows ? to work

<<setup,echo=F>>=
source('setup.R')
@ 

If you are of a certain age, you may remember the 1960s television show {\em
  Lost in Space}.  One of the characters, the Robot, was often
assigned guard duty.  Robot would march back and forth, just like a
soldier on guard duty.

But why?  Soldiers are ordered to march back and forth so that they
won't fall asleep; walking forces them to maintain a certain attention
to their duty.  Robot has no such need; his sensors and circuits can reliably
detect intruders without the marching.  Of course, the television
viewers wouldn't know this about robots.  Using the robot in the
manner of a soldier was a way to introduce new technology to people
with old mind-sets.

Now fast forward from the television of the 1960s to the classroom of
the 21st century.  Students have computers.  They use statistical
software packages to do calculations.  But what calculations are they
doing?  Sample means, sample proportions, differences between means.

This Robot walking back and forth.  A way to use new technology with
old mind-sets.  But it's the professors, not the students, with the
old mind-sets.  The students are open to new things.  They don't need
to know that, once upon a time, it was a feat to get a machine to add
up a column of numbers.

We professors were educated at a time when the tools for inverting a
matrix were paper and pencil, when doing least squares problem involved
a strange thing called a ``pseudo-inverse'' that you might learn in
your fourth or fifth semester of university-level mathematics.  But, now, least squares
problems are no more difficult or time consuming to the human than
square roots or addition.  We just have to learn to use the tools in
the right way.  

Or, rather, we have to show our students what are the
basic operations that are important for statistical reasoning in an
age of modern computation.  Not marching back and forth, like robot
soldiers, computing sums of columns of numbers, but thinking about how to model the
potentially rich interplay among multiple variables.

The standard approach to introductory statistics is based on a few
simple operations: adding, squaring/square-rooting to finding means and standard deviations; counting to
find proportions and medians/quantiles.  These operations are
(supposed to be) familiar to students from high-school mathematics.

Here we'll examine the consequences of adding in a new basic operation
that is not in the traditional high-school curriculum, but which is
actually quite consistent with the ``Common Core'' curriculum being
introduced by many U.S. states.

\authNote{DTK: Need citation to the common core.  Common Core State
  Standards Initiative, ``Common Core State Standards for
  Mathematics'', \texttt{http://www.corestandards.org/}}

The operation is fitting multivariate linear models.  Modern software
makes it no more arduous than finding a mean or standard deviation.
Our emphasis here will be on how to introduce the conceptual
foundations --- using just high-school math and simple extensions
largely specified already in the ``Common Core'' --- that make the concept
of modeling accessible.

\section{The Mathematical Foundations}

A standard part of the high-school curriculum is the equation of a
straight line: $y = m x + b$.  Many students will recognize that $m$
is a slope and $b$ is the $y$-intercept.  

It's helpful to move them a bit beyond this:
\begin{itemize}
\item Emphasize the ``function'' concept.  A function, of course, is a
  relationship between an input and an output.  Generalize this, so
  that students are comfortable with using names other than $x$ as the
  input and
  $y$ as the output.  For example, 
  $$\mbox{height} = f( \mbox{age} ) = 3\ \mbox{age} + 20$$.
  
\item Introduce the idea of functions of non-numeric variables, for example:
$$\mbox{height} = g( \mbox{sex} ) = \left\{ \begin{array}{ll}
      67 & \mbox{for sex $=$ female}\\
      71 & \mbox{for sex $=$ male}\\
      \end{array}\right.$$
   
\item Generalize the idea of a function to include having more than
  one input, for instance

$$\mbox{height} = h( \mbox{age}, \mbox{sex} ) = -2\ \mbox{age} + 
      \left\{ \begin{array}{ll}
      67 & \mbox{for sex $=$ female}\\
      71 & \mbox{for sex $=$ male}\\
      \end{array}\right.$$

\item  De-program your students from thinking that there is one, and
  only one, correct formula for a relationship.  Introduce the idea of a function as a
  description of a relationship and that there can be different descriptions of the same
  relationship, all of which can be right in some ways.  Different
  descriptions can include some details and exclude others. Such
  descriptions are called ``models.''  The above models
  $f(\mbox{age})$, and $g(\mbox{sex})$ and $h(\mbox{age}, \mbox{sex})$
  give different outputs for the same inputs.  For example, for a male
  of age 10, according to the models, the height is variously $f(10) =
  50$ or $g(\mbox{male}) = 71$ or $h(10, \mbox{male}) = 51$.
  
\item  Many useful models don't give exactly the right output for
  every case.  Not every 10-year old male has the same height.  The
  difference between what a model says, and what the value is for an
  actual case, will not in general be zero.  The ``residual'' is the difference between
  the actual value for a given 10-year old male, say Ralph, and what
  a given model says about 10-year old males in general.  The
  residuals give important information about how good a model is.
   
\end{itemize}


\section{The Language of Models}

There is a language for defining models that is different from
writing down an algebraic formula.  The language has just a few components:
\begin{itemize}
  \item The response variable.  This is the output of the model function,
    \VN{height} in the above examples.
  \item The explanatory variables.  These are the inputs to the
    model function: \VN{age} and \VN{sex} in the above examples.
  \item Model terms.  Examples of model terms are explanatory
    variables themselves and a special term called the ``intercept.''
    There are a few others, but we'll deal with them as we come to them.
\end{itemize}

As an example, consider this simple formula:
$$ z = 7 + 3 x + 4.5 y.$$
The corresponding model description consists of the response variable
$z$, and three model terms: the explanatory variables $x$ and
$y$ and the intercept term.  In the modeling language, it would be written:
\begin{quotation}
\centerline{\model{z}{1 + x + y}}
\end{quotation}
Notice that instead of the $=$ sign, we are using the $\sim$ sign.
Also, notice that the specific coefficients $7$, $3$, and $4.5$ are
missing.  The model description is a kind of skeleton for describing
the shape of the model.  It's like saying: ``We want a straight line
model,'' rather than giving the complete specification of the formula.

The process of finding a specific formula to make a model match the
pattern shown by data is called ``fitting the model to the data.''  

To see how different model specifications correspond to different
``shapes'' of models, consider the following data of world-record
times in the 100-meter free-style race.

<<read-swim,echo=false>>=
swim = read.csv("http://www.macalester.edu/~kaplan/ISM/datasets/swim100m.csv")
swim.plot = function(col='black',show.legend=TRUE){
plot( time ~ year, pch=21-2*as.numeric(sex), cex=1.4,xlim=c(1899,2010),
     cex.lab=1.3, cex.axis=1.3,data=swim,xlab='Year',
     ylab='Record Time (secs)',col=col)
  if( show.legend) legend(1980,92, legend=c("Women","Men"),pch=c(19,17),cex=1.3)
}
@ 


<<swim-data-raw,fig=true,pdf=true,include=true,width=6,height=4,echo=false>>=
swim.plot( )
@ 


You can see the steady improvement in records over the decades from
1900 to the present.  Men's times are somewhat faster than women's.

Now to build some models.



\section{Finding Formulas from Data}

\section{Genetics before Genes}

About Galton's data.

\section{Bias in Sampling?  Choosing library books off a shelf?}

\section{Taking Your Class for a Random Walk}

\section{Biostatistics example: gene sequencing?}

\section{The perils of multiple comparisons}

\section{The sampling distribution of R2 and F under the null hypothesis}

\section{A demonstration of shuffling and how it implements the null hypothesis}

\section{Working through an ANOVA calculation by hand}

\section{Sampling bias in survival studies (with a simulation)}


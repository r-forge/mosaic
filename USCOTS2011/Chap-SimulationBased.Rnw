

\chapter{Simulation Based Inference}

\SweaveOpts{prefix.string=figures/fig}  % location of 
\SweaveOpts{highlight=T}    % not sure this does anything unless we use pgfSweave
\SweaveOpts{tidy=F}         % keep.source probably disables this
\SweaveOpts{pdf=T}          % use pdf for graphics
\SweaveOpts{strip.white=T}  % remove blank lines at beginning and end 
\SweaveOpts{keep.source=T}  % keeps formatting from original; allows ? to work
\SweaveOpts{width=3}
\SweaveOpts{height=2}
\SweaveOpts{cache=T}


<<setup,echo=F>>=
source('setup.R')
@ 

\section{Simulation and Randomization with the  \texttt{mosaic} Package}

Software environments almost always provide an important feature:
\authNote{Why use itemize when you want to enumerate?  If we don't like the 
display of enumerate, we should reset the defaults.  I'm switching to enumerate.
---rjp.  DTK says, ``Thanks.''  I didn't know about the saveenumi and
reuseenumi.  Very helpful.  One of many things I'm learning.  }
\begin{enumerate}
  \item They allow potentially complicated operations to be
    packaged with a simple interface, making them easy to use.
	\saveenumi
\end{enumerate}
An environment that is integrated with a programming language provides
an additional capability:
\begin{enumerate}
\reuseenumi
  \item New operations can be constructed from existing ones.
\end{enumerate}

Any proper statistics environment should offer the ease of use of
(1).  \R, like many packages, offers a large set of pre-packaged
operations.  But, as a modern programming language, \R\ offers the
advantage of making (2) available as well as (1).  In addition to
enabling \R\ to provide ready access to new forms of state-of-the-art
computing, the programming-language features of \R\ also enable the
operations in (1) to be de-constructed and presented to students in a
transparent and intelligible way that reveals the underlying logic.

One of the important goals of the \verb!mosaic! package is to provide
elementary commands that can be easily strung together by novices
without having to master the esoteric aspects of programming.  This
chapter will describe a few such key operations and how they connect
to one another: random sampling and resampling, replication of random
trials, summarizing the results of multiple trials.  As you will see,
the \verb!mosaic! operations allow students to implement each of the
operations in what George Cobb calls the ``3 Rs'' of statistical
inference: Randomization, Replication, and Rejection \cite{USCOTS-cobb-2005}.
By putting the 3 Rs together in various ways, students learn to
generalize and internalize the logic of inference, rather than just
following formulaic methods.  
More examples of the use of R as this sort
of \emph{sandbox} for experimentation can be found in \cite{hort:brow:2004}.

There's an interesting discussion of the role of simulation in \cite{speed:2011}, where
he notes the changing role of simulation.  It used to be:
\begin{quote}
something that people did when they can't do the math. $\ldots$ It now seems that we
are heading into an era when all statistical analysis can be done by simulation.
\end{quote}



\subsection{Sampling and Resampling}

Arguably, the most important operation in statistics is sampling:
ideally, selecting a random subset from a population.  Regrettably,
sampling takes work and time, so instructors tend to de-emphasize the
actual practice of sampling in favor of theoretical descriptions.
What's more, the algebraic notation in which much of conventional
textbook statistics is written does not offer an obvious notation for sampling.

With the computer, however, these efficiency and notation obstacles
can be overcome.  Sampling can be placed in its rightfully central
place among the statistical concepts in our courses.

In doing so, why not start right off with a sample from a population?
One example is the population of US states:

\authNoted{DTK: I'm reading it in this awkward way until I can add the dataset to
  the \verb!mosaic! package.  In the \verb!mosaic! package, the current ``SAT'' data
set has just one variable.  My view is that all such data sets should
be data frames.  But I understand the virtue of having a few that
students can use without the \$ notation.}%
\authNote{RJP: I added the data and cleaned it up a bit.
All you need do is add documentation.}%

\begin{example}[ (Sampling from 50 States)]

Students in the US will know that there are 50 states:
<<>>=
nrow(SAT)
@ 


\authNoted{DTK: Danny should fix the SAT data set so that states like
  ``New York'' don't have a comma inserted in place of the space.

  RJP:  done.}

One of the variables to be measured from the population is the
student/teacher ratio.  There are two equivalent ways to calculate this quantity:
<<>>=
mean(SAT$ratio)
@ 

\authNote{DTK: There's another style that we could push.  I'm
  interested in your reaction to it. This has the advantage of segregating the
  randomization part from the calculation part.  
  I'll try using this style in the following, and we
can easily change back to a more conventional style if it gets irritating.}

<<>>=
with(SAT, mean(ratio))
@ 

Every student who does this calculation will get the same result.
But if each student takes a random sample (without replacement) of, say, 4 states:
\authNote{RJP: If we want a sample, let's call it a sample, not a shuffle.  I've changed
the \R\ code throughout this section.}%
<<>>=
sample(SAT, 4)
@ 

Then each student gets a potentially different result.

Doing this with a classroom of students will immediately elicit a
realization that the random sample is exactly that.  Ask the class:
Who got California?  (There should be one or two in a class of 20.)
Who got Alabama?  Did anyone get the same state twice?
\authNote{RJP:
No one will get the same state twice with \verb!smaple()! or 
with \verb!shuffle()! under the default settings.  \verb!resample()! 
on the other hand, samples with replacement by default.
We could use \verb!deal()! here, but why mix metaphors when you are talking
about sampling?
}
\end{example}

\begin{problem}[stars=2]
 (This isn't a question for beginning students, just one
for developing instructor capabilities.)  
How would you demonstrate that there are 50
different states, rather than just 50 cases that might include
duplicates?  
Try each of the following
and see if you can understand what's going on in each of them:
\authNote{We had \verb!echo=false! here, but I think we mean \verb!eval=false! ---rjp}%
<<eval=false>>=
with(SAT, table(state))
length(with(SAT, table(state)))
with( SAT, table(table(state)))
table(table(SAT$state))
@ 
\end{problem}

The value of some measure for the population is, of course, called the ``population
parameter.''  
\authNote{RJP: Why use scare quotes here?  Use \verb!\term{}! to highlight terms.  We 
can define that to be boldface font or some such.}%
But for a sample, it's a ``sample statistic.''  It's
worthwhile to make the distinction concrete:
<<>>=
with(SAT, mean(ratio))
with(sample(SAT,4), mean(ratio))
@ 

Good questions to ask in class at this point: Who got a sample
statistic that's bigger than the population parameter?  (About half
the class will say yes.)  Who got a sample statistic that's exactly
equal to the population parameter?  (It's unlikely that there will be a match.)
\authNote{Is it worth the effort to write a few macros that place marginal
notes alerting the reader to things like ``questions for students'', 
``just for instructors'', ``R pitfalls'', etc.  
Perhaps we could use the data set identifier from Danny's book
as a model?  I'm thinking it would be nice to make it easier for users to locate
various types of material.}


At this point, it's helpful to point out that the reason we take
samples is because it can be difficult or impossible to measure every
member of the population.  Of course, it's not so difficult if your
population is the 50 US States, but what if your population were the
whole set individual people living in the US: that's over 300,000,000
people!  


\subsection{Taking Randomness Seriously}

\label{sec:taking-randomness-seriously}

It's tempting to think that ``random'' means that anything goes.  This
is, after all, the everyday meaning of the word.  In statistics,
though, the point of randomness is to get a representative sample, or
to assign experimental treatments in some fairly even way.   
\authNote{RJP: 'fairly even' is pretty vague and potentially misleading.
Randomness is really about having some knowledge about the variability.
Other methods may have less variability, but if we don't know how to quantify it
and if it is miscentered (biased), it doesn't do us any good. }%
Using
randomness properly means taking care and being formal about where the
randomness comes from.  When you finally submit the research paper you
have written after your long years of labor, you don't want it to be
rejected because the reviewer isn't convinced that what you called
``random'' was really so.

First, to demonstrate that the sort of computer-generated random
sampling produces reasonable results.  Perhaps it's a little too early
in the course to talk about sampling distributions, but you certainly
can have your class generate random samples of a reasonable size and
show that sample statistics are reasonably close to the population parameter.  
\authNote{RJP: samples are not close to parameters, statistics are.  I changed
'they' to 'sample statistics'}%
For
instance, have each student in your class do this:
<<>>=
with(SAT, mean(ratio))
with(sample(SAT,25), mean(ratio))
@ 
Or, each student can repeat the trial several times:
<<>>=
do(10)*with(sample(SAT,25), mean(ratio))
@ 
This raises the question, naturally enough, of what ``reasonably
close'' means, and what it means to measure ``how close.''
Eventually, that might lead you into consideration of differences,
mean differences, mean square differences, and root mean square
differences: the standard deviation and variance.  But even before
encountering this formality the
students should be able to see that there is nothing systematically
wrong with the answers they get from their random samples.  Some are too high compared to the
population parameter, some are too low, but as a group they are pretty
well centered.

\subsection{Illuminating Sampling Mistakes}
\authNote{RJP: The word 'illuminating' has two meanings intentionally -- but feel free to 
change if that's too cute.}%
It's helpful at this point to have some examples where an informal or careless
approach to randomness leads to misleading results.

Here are some that can be visually compelling:
\begin{enumerate}
\item Select books off a library shelf and count how many pages 
are in each book.
  When students pick books haphazardly, they get a systematic
  over-estimate of the page length.  
  
  A random sample of 25 is
  sufficient to see this for the large majority of students in the
  class.  Going up to 50, though tedious, makes the result even more compelling.
  \authNote{RJP:  I moved Danny's comment (Introduce the R app that let's
  students do this with the mouse.)  to this note.  
  It's too late in the game to put hard to spot cruft into the document
  that we need to remember to remove later.}%
  
\item A simulation of a real-world study of Alzheimer's disease.  The
  sampling process was to go to nursing homes on a given day and
  randomly pick out patients who were diagnosed with Alzheimers.
  Their files were flagged and, after a few years, the data were
  examined to find the mean survival time, from when they entered the
  nursing home to when they died.  
  
\authNote{We can easily write an app that generates admission and
  death dates as Julian dates, and let's the student pick a random day
  of the year for the sampling.  A graphic shows how the random day
  tends to select those who are at the nursing home for a long time,
  rather than equally picking all patients.  Students can then compare
  the mean for the population to the mean for their haphazard sample.}

  \item Sample people and find out how many siblings are in their family
  (including themselves).
  Use this data to estimate family size.  Since larger families have more
  children, we will over-sample larger families and over-estimate 
  family size.  Section~\ref{sec:fake-families} demonstrates how
  to fake a data set for this activity.  But for the present, we can
  use the \texttt{Galton} dataset and pretend to take a sample of,
  say, size 300 kids from the whole group of children that Galton had assembled.  
 
<<>>=
data(Galton)
with(sample(Galton,300), mean(nkids))
with(sample(Galton,300), mean(nkids))
do(10)*with(sample(Galton,300), mean(nkids))
@ 
The samples all give about the same answer: about 6 kids in a family.
But this is misleading!  The case in the \texttt{Galton} data is a
child, and families with more children have their family size
represented proportionally more.  


\authNote{DTK: To be added to the style guidelines????  I like to have a macro that typesets variable names in
  a consistent way, e.g., a sans-serif font.  It doesn't matter so
  much what the typestyle is, so long as we can be consistent.  I
  suggest \verb+\VN{}+ as the markup.  We need a similar markup for
  data sets and for operators (so that we can make an index of operators.}
There are several ways to convert the \texttt{Galton} to a new data
set where the case is a family.  The key information is contained in
the \VN{family} variable, which has a unique value for each family.
The \texttt{duplicated} function identifies levels that are repeats of
earlier ones and so the following will do the job (although is not
necessarily something that you want to push onto students):
<<>>=
families = subset(Galton, !duplicated(family))
@ 
Of course, you will want to show that this has done the right job.  Compare
<<>>=
nrow(families) 
@ 
to 
<<>>=
length(with(Galton, unique(family)))
@ 

Now check out the average family size when the average is across
families, not across kids in families:
<<>>=
with( families, mean(nkids) )
@ 


  (Note:  In a higher-level course, you could ask students
  to determine a method to correct for this bias.)
\authNote{Consider whether my little example is too advanced and
  should be moved to a higher-level course.  A good general policy for
  this workshop would be to ask the users to mark up places where the
  commands seem too difficult, or to highlight ``student use'' versus
  ``instructor demonstration.''  There are always students who are
  keen to see more, even if they aren't expected to master it.}

  \authNote{RJP:  build appropriate data frames for this and Alzheimer's examples.
  Do we want to show how to build these?  I say yes.  It would be good for instructors
  to see how these are built.  Those who are not interested can simply
  copy.  We can put this stuff into a \verb!demo()! for easy access.
  DTK: I agree.
  }

  \item
  Using \verb!googleMap()! (see Section~\ref{sec:googleMap}) 
  we can compare uniform sampling of longitude 
  and latitude with correct sampling that takes into account that the earth
  is a sphere, not a cylinder.
\end{enumerate}

In each of these cases, follow up the haphazard or systematically biased 
sample with a formally generated sample (which is faster to do, in any event) and show that
the formally generated sample gives results that are more
representative of the population than the haphazardly sampled data.


\subsection{Sampling From Day 1 with \texttt{rflip()}}

Since tossing a coin is one of the most familiar examples of randomness, we've added to
\verb!mosaic! a function that facilitates simulating coin tosses. 
<<>>=
rflip(20)
as.numeric(rflip(20))       # just count how many heads
@


In addition to the Lady Tasting Tea activity (Section~\ref{sec:lady-tasting-tea}),
other activities can be devised to help students develop a sense for randomness 
and variability.

\begin{enumerate}
\item If you flip a coin 100 times, how often will you get exactly 50 heads?
Between 45 and 55 heads?  Between 40 and 60 heads?
\item
If you flip a coin 20 times, how long is the longest run of either heads or tails typically?

This can be part of demonstration where you have students write down strings of H and T
that ``look random'' and another set based on actual coin tosses.  Then see if you can 
tell them which are which.

\item
\verb!rflip()! allows you to flip biased coins as well.  Let's simulate a 90\%
free throw shooter shooting 10 free throws.
\end{enumerate}
<<>>=
rflip(10, prob=.9)
@

\authNote{Add some more of these.  Should we include examples where the coin is 
biased -- free throw shooting comes to mind, for example.  Also the tipping 
and spinning pennies activities.}%

\subsection{Basic Sampling With \texttt{sample()}, \texttt{resample()}, and Cousins}

<<>>=
rdata(20, 1:4)              # samples with replacement
@

\authNote{\texttt{rdata()} could go in this section or in the next.  It serves as a 
transition from sampling from data to sampling from an abstract distribution.}%

\subsection{Sampling from Distributions}

%But better to sample from less abstract quantities and use these to
%generate some of the distributions that are used later.


<<>>=
rnorm(20, mean=500, sd=100) 
rexp(20, rate=1/5)
@

<<>>=
# These are equivalent
sample(cards, 5)
deal(cards, 5)
@
<<>>=
# These are equivalent
sample(cards) 
shuffle(cards) 
@

<<echo-false>>=
set.seed(123)
@
<<>>=
# These are equivalent.  Note the order of the arguments.
sample(HELP, 2)
rdata(2, HELP)
@

<<echo-false>>=
set.seed(123)
@
<<>>=
# These are equivalent; sampling with replacement now
resample(HELP, 2)
rdata(2, HELP, replace=TRUE)
@

\subsection{\texttt{do()}}
The heart of a simulation is doing something over and over.  
The \verb!do()! function simplifies the syntax for this and improves 
the output format (usually returning a data frame).

<<>>=
do(5)*"hello"
@



\subsection{Generating Fake Data For Sampling Activities}
\label{sec:fake-families}%
If you don't have actual data at your disposal, you can sometimes simulate data
to illustrate your point.  

\begin{example}[ (Fake Families)]
We'll simulate 5000 families using a Poisson distribution with a mean of 3
to generate family size.  (You can make up whatever elaborate story you like
for the population you are considering.)
\InstructorNote{This example is intended for instructors, not for students.}

<<>>=
families <- data.frame(familyid = 1:5000, children = rpois(5000,3))
@

Now we generate the people in these families
<<>>=
people <- data.frame( 
		familyid = with(families, rep( familyid, children)),
		sibs = with(families, rep( children, children)) 
		)
@

<<>>=
with(families, mean(children))
with(people, mean(sibs))
@
If the result is mysterious, the following tables might shed some light.
<<>>=
with(families, table(children))
with(people, table(sibs))
@
\end{example}

Once built, you can provide these data frames to your students for a sampling exercise.
(See \ref{sec:distributingData}.)


\section{Empirical p-values}
\subsection{Lady Tasting Tea}
If you don't use the Lady Tasting Tea as a course starter 
(Section~\ref{sec:lady-tasting-tea}), you can use it as 
an introduction to testing a proportion.

\subsection{Golfballs in the Yard}

<<>>=
golfballs <- c(137, 138, 107, 104) 
table(rdata(486, 1:4))
@
<<>>=
rgolfballs <- do(2000) * table(rdata(486, 1:4))
@


\begin{center}
<<golfballs-max,fig=true>>=
print(statTally( golfballs, rgolfballs, max))
@
\end{center}

\begin{center}
<<golfballs-range,fig=true,height=1.7>>=
print(statTally(golfballs, rgolfballs, function(x) { diff(range(x)) }))
@
\end{center}

\begin{center}
<<golfballs-var,fig=true,height=1.7>>=
print(statTally( golfballs, rgolfballs, var))
@
\end{center}


\subsection{Comparing Two Means}

\label{sec:comparing-two-means}

In this section we show two ways to see if the mean ages of men and women in the HELP study 
are significantly different.

\subsubsection{Using \texttt{aggregate()}}

While \verb!summary()! from \verb!Hmisc! makes a nice visual display
of summary statistics by groups,
<<help101>>=
summary(age ~ sex, data=HELP, fun=mean)
@
the \verb!aggregate()! function stores its results in a data frame that lends 
itself better to further computation.
 
<<help102>>=
aggregate(age ~ sex, HELP, FUN=mean)
@
A natural test statistic for comparing two means is the difference in sample means,
which we can calculate using
<<help103>>=
diff(aggregate( age ~ sex, HELP, mean )$age)                  # actual
with(aggregate( age ~ sex, HELP, mean ), diff(age))           # actual
@

Using \verb!do()! and \verb!shuffle()! we can now compare
\Sexpr{round(diff(aggregate(age ~ sex, HELP, mean )$age),2)} with
the values we obtain if we randomly shuffle the \verb!female! variable.

<<help104>>=
do(1) * diff(aggregate(age ~ sex, HELP, mean)$age)                    # actual
do(2) * diff(aggregate(age ~ shuffle(sex), HELP, mean)$age)           # shuffled
do(2) * c(diff=diff(aggregate(age ~ shuffle(sex), HELP, mean)$age))   # shuffled & renamed
@


If we increase the number of random shufflings, we get an approximate sampling distribution
that we can use to compute an empirical p-value.

<<two-sample,width=4>>=
set.seed(123)
test.stat <- diff(aggregate(age ~ sex, HELP, mean )$age)
rtest.stats <- do(1000) * c(diff= diff(aggregate(age ~ shuffle(sex), HELP, mean )$age))
rtest.stats <- rtest.stats$diff
@
<<two-sample-pval>>=
table(rtest.stats >= test.stat)
mean(rtest.stats <= test.stat)            # compute proportion of extreme statistics
@
With an empirical p-value of approximately \Sexpr{round(mean(rtest.stats >= test.stat),3)}, 
there is no reason to reject the null hypothesis that the mean
age is the same for men and women.

We can use histograms or dotplots to display the sampling distributions graphically.
\begin{center}
<<two-sample-hist,fig=true,width=5>>=
xhistogram(~ rtest.stats, v=test.stat, groups=rtest.stats >= test.stat)      
@
\end{center}

\begin{center}
<<two-sample-dot,fig=true,width=5,height=2.2>>=
test.stat <-  diff(aggregate(age ~ sex, HELP, mean )$age)
rtest.stats <- do(200) * c(diff= diff(aggregate(age ~ shuffle(sex), HELP, mean)$age))
dotPlot(~ diff, rtest.stats, n=40, groups=rtest.stats$diff >= test.stat, pch=16 )
ladd(panel.abline(v=test.stat))                 # add a vertical line
@
\end{center}

\subsubsection{Using \texttt{t.test()}}

If you prefer to teach randomization methods as an alternative to traditional 
methods when the assumptions of those procedures are violated, you can use 
simulation to approximate the sampling distribution of the usual test statistic.
<<>>=
data.t <- stat(t.test( age ~ sex, HELP)); data.t
simulated <- do(1000) * stat(t.test(age ~ shuffle(sex), HELP))
table(simulated$t >= data.t)
@
\begin{center}
<<t-test-simulation-histogram,fig=true,width=5>>=
xhistogram(~ t, simulated, groups=t >= data.t, v=data.t)
@
\end{center}



\subsubsection{Using \texttt{lm()}}
An alternative approach to comparing two means is based on the use of linear models. 

\begin{example}
In this example,
\R\ will automatically convert the \verb!sex! factor into 0's and 1's, so the slope parameter in the model
is the difference in the means.  
<<>>=
lm(age ~ sex, HELP)                        # actual data
@
The \verb!do()! function conveniently stores the values of the 
estimated parameters that result from fitting with \verb!lm()!, so it is relatively easy to obtain
the sampling distribution for any of these estimated parameters.
<<>>=
do(1) * lm(age ~ sex, HELP)                # actual data
do(2) * lm(age ~ shuffle(sex), HELP)       # shuffled data
@

Since this is just another way of computing the difference in the sample means, the two approaches
are equivalent.  If we reset the random seed, we get exactly the same simulation results.
<<>>=
set.seed(123)
simulated <- do(1000) * lm(age ~ shuffle(sex), HELP) 
test.stat <- (do(1) * lm(age ~ sex, HELP) )$sexmale -> test.stat
rtest.stats <- simulated$sexmale
table(rtest.stats >= test.stat)
mean(rtest.stats >= test.stat)
@
\end{example}

\section{The Multi-World Metaphor for Statistical Inference}

Statistical inference is hard to teach.\footnote{Here we are considering only the
  frequentist version of inference.  The Bayesian approach is
  different and has different features that make it hard to teach and
  understand.  In these notes, we will be agnostic about frequentist
  vs Bayesian, except to acknowledge that, for good or bad, the
  frequentist approach is vastly dominant in introductory statistics courses.}  
Often, instead of teaching
the logic of inference, we teach methods and techniques for
calculating the quantities used in inference: standard errors,
t-statistics, p-values, etc.  

Perhaps because students don't understand the logic, they have strong
misconceptions about confidence intervals and, especially, about
p-values.  For example, even among professional scientists, the
mainstream (mis)-understanding of p-values is that they reflect the
probability that the null hypothesis is correct.

Part of the reason why statistical inference is hard to grasp is that
the logic is genuinely hard.  It involves contrapositives, it
involves conditional probabilities, it involves ``hypotheses.''
And what is a hypothesis?  To a scientist, it is a kind of theory, an idea of how
things work, an idea to be proven through lab or field research or the
collection of data.  But statistics hews not to the scientist's but to
the philosopher's or logician's rather abstract notion 
of a hypothesis: a ``proposition made as a basis for reasoning, without any
assumption of its truth.''  (Oxford American Dictionaries)  What kind
of scientist would frame a hypothesis without some disposition to
think it might be true?  Only a philosopher or a logician.

To help connect the philosophy and logic of statistical hypotheses
with the sensibilities of scientists, 
it might be helpful to draw from the theory of kinesthetic learning
--- an active theory, not a philosophical proposition --- that learning is enhanced by
carrying out a physical activity.
Many practitioners of the reform style of teaching statistics engage
in the kinesthetic style; they have students draw M\&Ms from a bag
and count the brown ones, they have students walk randomly to see how
far they get after $n$ steps, they toss a globe around the classroom
to see how often a students' index finger lands in the ocean.
\authNote{Get the citations to these.  The first, I think, is from
  Workshop statistics.  The second can refer to DTK's CAUSEWEB talk.
  The third is, I think, in Deb Nolan's ``Tricks'' book.}

To teach hypothesis testing in a kinesthetic way, you need a physical
representation of the various hypotheses found in statistical
inference.  One way to do this involves not the actual physical
activity of truly kinesthetic learning, but concrete stories of making
different sorts of trips to different sorts of worlds.  To that end,
we humbly offer physical analogs of the various hypotheses: the planets.

Of course, the planet that we care about, the place about which we
want to be able to draw conclusions, is not at all hypothetical.  It's
Planet Earth:

\centerline{\includegraphics[width=1.2in]{images/planet_earth.png}}

This is not a hypothesis; it's reality.  But it's a big place and
complicated and it's hard to know exactly what's going on.  So, to
deal with the limits of our abilities to collect data and to
understand complexity, statistical inference involves a different set
of planets: ones that are entirely hypothetical constructions.  They resemble Planet
Earth in some ways and not in others.  But they are simple enough that we know
exactly what's happening on them: they are ``proposition[s] made as a
basis for reasoning, without any assumption of [their] truth.''  These
hypothetical planets are:

\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=1.2in]{images/planet-sample.png} &
\includegraphics[width=1.2in]{images/venus.png} &
\includegraphics[width=1.4in]{images/planet-alt.png} \\
Planet Sample & Planet Null & Planet Alt\\
\end{tabular}
\end{center}

Planet Sample is populated entirely with the cases we have collected
on Planet Earth.  As such, it somewhat resembles Earth, but many of
the details are missing, perhaps even whole countries or continents or seas.

Planet Null is a boring planet.  Nothing is happening there. Express
it how you will: The different groups all have the same mean values;
The model coefficients are zero; Different variables are unrelated to
one another.  Dullsville.  But even if it's dull, it's not a polished
billiard ball that looks the same from every perspective.  It's the
clay from which Adam was created, the ashes and dust to which man
returns.  But it varies from place to place, and that variation is not
informative. It's just random.

Finally, there is Planet Alt.  This is a cartoon planet, a caricature,
a simplified representation of our idea of what's going on, our theory
of how the world might be working.  It's not going to be exactly the
same as Planet Earth.  For one thing, our theory might be wrong.  But
also, no theory is going to capture all the detail and complexity of
the real world.

In teaching statistical inference in a pseudo-kinesthetic way, we use
the computer to let students construct each of the hypothetical
planets.  Then, working on that hypothetical planet, the student can
carry out simulations of familiar statistical operations.   Those
simulations tell the student what they are likely to see {\em if they
  were on that planet}.   

For the professional statisticians, the planet metaphor is
unnecessarily.  The professional has learned to keep straight the
various roles of the null and alternative hypothesis, and why one uses
a sample standard error to estimate the standard deviation of the
sampling distribution from the population.  But most students find the
array of concepts confusing and make basic categorical errors.  The
concrete nature of the planets simplifies the task of keeping the
concepts in order.  For instance, Type I errors are only about being
on Planet Null.  You have to be on Planet Alt to make a Type II
error.  And, of course, no matter how many (re)samples we make on
Planet Sample, it's never going to look more like Earth than the
sample itself.

\section{The Sampling Distribution}

Section \ref{sec:taking-randomness-seriously} shows some examples of
drawing random samples repeatedly from a set of data.  To discuss the
sampling distribution, it's helpful to make clear that the sampling
distribution refers to results from the {\bf population}.  In the planet
metaphor, this means that the samples are drawn from Planet Earth:

\centerline{\includegraphics[width=1.2in]{images/planet_earth.png}}

It can be a challenge to find a compelling example where you have the
population, rather than a sample, in hand.  Sports statistics provide
one plausible setting, where
you have, for example, the names and attributes of every professional
player.  The ability to sample from Earth mapping software
\authNote{Reference needed here to the Google maps example.} provides
another compelling setting.  

As one example, we will take the complete list of results from a
running race held in Washington, D.C. in April 2005 --- the Cherry
Blossom Ten-Mile run.  The data set gives the \VN{sex}, \VN{age}, and \VN{net} running time
(start line to finish line) for each of the 8636 runners who
participated: the population of runners in that race.

\authNote{I changed \verb!our.population! to \verb!population!.  If you really like the old
name, you can change it back.  --rjp}
<<>>=
population <- TenMileRace
nrow(population)
@ 

If you have the whole data set on the computer, there's little point
in taking a random sample.  But the point here is to illustrate the
consequences of taking a sample.  So, imagine that in fact it was
difficult or expensive or dangerous or destructive to collect the data on a runner,
so you want to examine just a small subset of the population.

Here's a sample of 100 runners:
<<run-sample>>=
mysamp = sample(population, 100)
@ 

With that sample, you can calculate whatever statistics are of
interest to you, for instance:
<<>>=
with(mysamp, mean(age))
with(mysamp, sd(age))
with(mysamp, mean(sex=="F"))
lm(net ~ age + sex, data=mysamp)
@ 

These various numbers are informative in their own way about the
sample, but it's important to know how they might relate to the
population.  For example, how precise are these numbers?  That is, if
someone had collected a different random sample, how different would
there results be likely to be.

Given that we have the population in hand, that we're on Planet Earth,
we can go back and collect such samples more times and study how much
they vary one to the other.  We'll do this by replacing
\texttt{mysamp} with a command to sample anew from the population.

<<>>=
with(sample(population,100), mean(age))
with(sample(population,100), sd(age))
with(sample(population,100), mean(sex=="F"))
lm(net ~ age + sex, data=sample(population,100))
@ 

Slightly different results!  To quantify this, repeat the sampling
process many times and look at the distribution: the {\bf sampling distribution}.

<<run-samp-dist>>=
s1 = do(500)*with(sample(population,100), mean(age))
s2 = do(500)*with(sample(population,100), sd(age))
s3 = do(500)*with(sample(population,100), mean(sex=="F"))
s4 = do(500)*lm(net ~ age + sex, data=sample(population,100))
@ 

You can display the sampling distribution in several ways: histograms,
box-and-whisker plots, density plots.  This is worth doing with the
class.  Make sure to point out that the distribution depends on the
statistic being calculated: in the above examples, the mean age, the standard deviation of
ages, the fraction of runners who are female, the relationship between
running time and \VN{sex} and \VN{age}.  (We'll get to the dependence
on sample size in a little bit.)

Insofar as we want to be able to characterize the repeatability of the
results of the sampling process, it's worth describing it in terms of the spread of
the distribution: for instance, the standard deviation.  Of course, when talking
about a sampling distribution, we give another name to the standard
deviation: the {\em standard error}.   Actually calculating the
standard error may perhaps solidify in the students mind that it is a
standard deviation.

<<>>=
sd(s1)  # mean age
sd(s2)  # sd of age
sd(s3)  # fraction female
sd(s4)  # relation of running time to age and sex
@ 

\begin{example}
This is an activity to carry out in class, with each student or pair
of students at a computer.

\begin{enumerate}
  \item Make sure you can replicate the calculations for the standard
    error of one of the statistics in the ten-mile-race example with a
    sample of size $n=100$.  Your reproduction won't be exact, but it
    should be reasonably close.
  \item Now repeat the calculations, but use sample sizes that are
    larger.  From $n=100$, increase to $n=400$, then $n=1600$, then $n=6400$.
    How does the standard error depend on $n$?  Does larger $n$
    lead to a bigger or smaller standard error?  Which of these
    formulas most closely matches the pattern you see:
    \begin{enumerate}
      \item The standard error increases with $n$.
      \item The standard error increases with $\sqrt{n}$.
      \item The standard error gets smaller with increasing $n$ with
       the pattern $1/\sqrt{n}$.
      \item The standard error gets smaller with increasing $n$ with
       the pattern $1/n$.
   \end{enumerate}
    \item Use the pattern you observed to predict what will be the
      standard error for a sample of size $n=1000$.  Then carry out
      the actual simulation of repeated sampling using that sample size and compare your
      prediction to the result you actually got.
      
    \item In the above, you used \texttt{do(500)} replications of
      random sampling.  Suppose you use \texttt{do(100)} or
      \texttt{do(2000)} replications instead?  Do your results depend
      systematically on the number of replications?
 
 \end{enumerate}
\end{example}


\section{The Re-Sampling Distribution}

The sampling distribution is a lovely theoretical thing.  But if
really were so easy to replicate taking a sample over and over again,
wouldn't you just take a larger sample in the first place?  The
practical problem you face is that the sample you took was collected
with difficulty and expense.  There's no way you are going to go back
and repeat the process that was do difficult in the first place.

This is where our hypothetical Planet Sample comes in.

\centerline{\includegraphics[width=1.2in]{images/planet-sample.png}}

You create Planet Sample yourself.  You do this by taking your sample
using it to populate Planet Sample; it consists of many, many copies
of your sample.

To illustrate, here's a sample from the running population:
<<>>=
mysamp = sample(population, 100) # a sample from the population
@ 
In reality, of course, you wouldn't run a command.  You would go out
and do the hard work of randomly selecting cases from your population,
measuring their relevant attributes, and recording that data.  

But here, in order to make a point, we're going to pretend to be
sampling from the runners' population rather than doing the actual
calculations.  Why?  In order to show you how the results on Planet
Sample relate to the results on Planet Earth.  The idea is, of course,
to show you that the work you will do on Planet Sample corresponds to
what's happening on Planet Earth.

To drive home the point that Planet Sample just consists of many
copies of the data in the sample itself, here's a way to construct it explicitly:
<<making-planet-sample>>=
planet.sample = rbind(mysamp, mysamp, mysamp, mysamp, 
                      mysamp, mysamp, mysamp, mysamp,
                      mysamp, mysamp, mysamp, mysamp)
nrow(mysamp)
nrow(planet.sample)
@ 

Now you can go through the exact calculations you did to construct the
sampling distribution (on Planet Earth), but instead use the data from
Planet Sample:
<<>>=
with(sample(planet.sample,100), mean(age))
with(sample(planet.sample,100), sd(age))
with(sample(planet.sample,100), mean(sex=="F"))
lm(net ~ age + sex, data=sample(planet.sample,100))
@ 


This is just one sample from Planet Sample.  Just as easy to collect
many replications and example the resulting distribution.

<<run-planet-samp-dist>>=
ps1 = do(500)*with(sample(planet.sample,100), mean(age))
ps2 = do(500)*with(sample(planet.sample,100), sd(age))
ps3 = do(500)*with(sample(planet.sample,100), mean(sex=="F"))
ps4 = do(500)*lm(net ~ age + sex, data=sample(planet.sample,100))
@ 

And, then, summarize the resulting distribution:
<<>>=
sd(ps1)  # mean age
sd(ps2)  # sd of age
sd(ps3)  # fraction female
sd(ps4)  # relation of running time to age and sex
@ 

It's an affectation, and unnecessary, actually to populate Planet
Sample by repeated copying of the data in your sample.  Instead, you
can arrange things so that the copying is done during the sampling
process.  Do this by using the \texttt{resample} operator rather
than \texttt{sample}.  As the name suggests, \texttt{resample} takes a
sample from a sample --- re-sampling --- and does so with replacement
so that it's possible that some cases will be duplicated and some will
be left out entirely.

You can illustrate this easily by comparing the process of sampling
and resampling on a short series of numbers:
<<>>=
our.set = c(1,2,3,4,5,6)
our.set
sample(our.set)  # no replacement
sample(our.set)
sample(our.set)
resample(our.set) # with replacement
resample(our.set)
resample(our.set)
@ 

Going back to the previous example, it will suffice to show what the
statements look like for just one of the statistics.
<<>>=
ps1 = do(500)*with(resample(mysamp), mean(age))
sd(ps1)
@ 

This operation, on Planet Sample, isn't quite constructing the
sampling distribution.  This is for the simple reason that it is being
done on Planet Sample rather than Planet Earth.  To emphasize the
distinction, it's helpful to refer to the {\em resampling
  distribution} in contrast to the sampling distribution.

\begin{example}
Have your students compare the results they get from resampling of a
fixed sample of size $n$, to repeated draws from the population with
the same sample size $n$.

Emphasize that the resampling process is good for estimating the width
of the sampling distribution, but not so good for estimating the
center.  That is, the results on Planet Sample will generally compare
quite well to Planet Earth for the standard error, but the means of
the sampling distributions and the resampling distributions can be
quite different.

In a more advanced class, you might ask how big a sample is needed to
get a reasonable estimate of the standard error.

\end{example}

The question naturally arises, is the standard error estimated from
the resampling distribution good enough to use in place of the
standard deviation of the actual sampling distribution.  Answering
this question requires some solid sense of {\em what you are using the
  standard error for}.    In general, we use standard errors to get an
idea of whether the point estimate is precise enough for the purpose
at hand.  Such questions can be productively addressed on Planet Alt,
where we will journey after a short detour to that most boring of all
places, Planet Null.

\section{The Sampling Distribution Under the Null Hypothesis}

The Null Hypothesis is often introduced in terms of the values of
population parameters, e.g., ``The population mean is 98.6,'' or ``The
difference between the two group means is zero,'' or ``The population
proportion is 50\%.''  

Perhaps this is fine if all you want to talk about is means or proportions or
differences between means, as is so often the focus of an introductory
statistics course.  But instructors would like to think that their
students are going to go further, and that the introductory course is
meant to set them up for doing so. 

In the multi-planet metaphor, the Null Hypothesis is about a place
where variables are unrelated to one another.  Any measured
relationship, as indicated by a difference in sample means, a sample
correlation coefficient different from zero, non-zero model
coefficients, etc., is, on Planet Null, just the result of random
sampling fluctuations.  This formulation if very general and is not
hard for students to understand.  (Ironically, it doesn't work so
well for the very simplest null hypotheses that are about single-group
means or proportions, so it turns out to be easier to introduce the
null hypothesis with the
supposedly more complicated cases, e.g., differences in means or
proportions, regression coefficients, etc.)

\centerline{\includegraphics[width=1.2in]{images/venus.png}}

Like Planet Sample, Planet Null is a place that you construct.  You
construct it in a way that makes the Null Hypothesis true: destroying
relationships between variables.  You've already seen the process in 
Section \ref{sec:comparing-two-means}: randomization with \texttt{shuffle}.
The basic idea is to treat relationships as a mapping from the values
of explanatory variables in each case to a response variable.  By
randomizing the explanatory variables relative to the response, you
generate the world in which the Null Hypothesis holds true: Planet Null. 

Some examples:
\begin{itemize}
\item Do the foot widths differ between boys and girls, judging from
  the \texttt{KidsFeet } data?
<<kidsfeet-null>>=
data(KidsFeet)
lm( width ~ sex, data=KidsFeet ) # our sample
@ 
\authNote{DTK: What do you say we re-define display.lm in the mosaic
  package to get rid of the report about the Call.  It just takes up
  useless space.}
Looks like girls' feet are a little bit narrower.  But is this the
sort of thing we might equally well see on Planet Null, where there is
no systematic difference between boys' and girls' feet?
<<echo=false>>=
set.seed(133)
@ 
<<>>=
lm( width ~ shuffle(sex), data=KidsFeet ) # planet null
@ 
For this particular sample from Planet Null, the girls' feet are a
little wider than the boys'.  By generating a sampling distribution on
Planet Null, we can see the size of the relationship to be expected
just due to sampling fluctuations in a world where there is no relationship.
<<>>=
sn1 = do(500)*lm( width ~ shuffle(sex), data=KidsFeet ) # distribution on planet null
mean( abs(sn1[,2]) > abs(-0.4058) )  # a p-value
@ 
The value of $-0.4058$ observed in our sample is not very likely on
Planet Null.  This suggests that our sample was not collected from
Planet Null: we can reject the Null Hypothesis.

\InstructorNote{The \texttt{[,2]} means to take the 2nd column of \texttt{sn1}. }
\authNote{If we can suppress the shuffle in the output of do(), then
  we can just use the \$ notation for pulling out a column.}

\item Is the survival rate for smokers different from that for non-smokers?
<<smokers-null>>=
data(Whickham)
lm( outcome=="Alive" ~ smoker, data=Whickham ) # our sample
lm( outcome=="Alive" ~ shuffle(smoker), data=Whickham ) # planet null
sn2 = do(500)*lm( outcome=="Alive" ~ shuffle(smoker), data=Whickham ) # distribution on planet null
mean( abs(sn2[,2]) > abs(0.07538) )  # a p-value
@ 
If you're shocked to see that smoking is associated with greater
likelihood of being alive (7.5 percentage points greater!) and that
the data indicate that this is statistically significant, you should
be.  But the problem isn't with the calculation, which is the same one
you will get from the textbook formulas.  The problem is with the
failure to take into account covariates.  What's shocking is that we
teach students about p-values without teaching them about covariates
and how to adjust for them.


The big covariate here is \VN{age}.  It happens that in the Whickham
 data, younger people are more likely to smoke.  To see this you :
<<>>=
lm( smoker=="Yes" ~ age, data=Whickham ) # our sample
lm( smoker=="Yes" ~ shuffle(age), data=Whickham ) # our sample
sn3 = do(500)*lm(smoker=="Yes" ~ shuffle(age), data=Whickham ) # distribution on planet null
mean( abs(sn3[,2]) > abs(-0.00326) ) # a p-value
@ 
So, greater \VN{age} is associated with lower \VN{smoker} status.
And, of course, older people  are more likely to die.  Taking both factors together, it turns out
 that smokers are less likely to die, but that's because they are
 young.  
\item Let's make up for the deficiency in the above smoking example.
  One way to do this is to adjust for \VN{age} when considering the
  effect of \VN{smoker} status.  We'll consider this more in Chapter
  \ref{chap:Multivariate-early}, but for now, we'll just build the model.
  
<<>>=
glm( outcome=="Alive" ~ smoker + age, data=Whickham, family="binomial" ) # our sample
glm( outcome=="Alive" ~ shuffle(smoker) + age, data=Whickham, family="binomial" ) # planet null
# distribution on planet null
sn4 = do(500)*glm( outcome=="Alive" ~ shuffle(smoker) + age, data=Whickham, family="binomial" ) 
mean( abs(sn4) > abs(-0.2047) ) # a p-value
@ 
You can see that the coefficient on \texttt{smokerYes} is negative,
and that the p-value indicates significance.  So, smoking in these
data are associated with a lower probability of survival, but only
when adjusting for \VN{age}.

You might have noticed that the model built here was a logistic
model.  There's good reason to do that, since we are modeling
probabilities the value of which must always be between 0 and 1.  But
notice also that the logic of hypothesis testing remains the same:
construct a Planet Null by randomizing an explanatory variable with
respect to a response variable.
\end{itemize}

As this is being written, the US Space Shuttle is carrying out it's
last couple of missions before being retired.  For a few years, at
least, your students will know what you mean by ``Space Shuttle.''
You can help them remember the way to create Planet Null if, at the
cost of some self-dignity, you tell them, ``Take the Space Shuffle to
Planet Null.''


\section{The Sampling Distribution Under the Alternative Hypothesis}

NOTES: the first step is to construct planet Alt.



\section{Bootstrap Confidence Intervals}
\authNote{Should we interleave the confidence intervals and p-values or separate them like this? --rjp}%

\section{Power}

\section{Exercises, Problems, and Activities}

\shipoutProblems


\chapter{Simulation Based Inference}


\SweaveOpts{prefix.string=figures/fig}  % location of 
\SweaveOpts{highlight=T}    % not sure this does anything unless we use pgfSweave
\SweaveOpts{tidy=F}         % keep.source probably disables this
\SweaveOpts{pdf=T}          % use pdf for graphics
\SweaveOpts{strip.white=T}  % remove blank lines at beginning and end 
\SweaveOpts{keep.source=T}  % keeps formatting from original; allows ? to work
\SweaveOpts{width=3}
\SweaveOpts{height=2}
\SweaveOpts{cache=T}


<<setup,echo=F>>=
#setCacheDir("cache")
require(grDevices); require(datasets); require(stats); require(lattice)
require(fastR)
require(grid) 
require(mosaic)
trellis.par.set(theme=col.mosaic(bw=FALSE))
trellis.par.set(fontsize=list(text=9))
options(keep.blank.line=FALSE); options(width=100)
xyplot <- function(...) { print(lattice::xyplot(...)) }
bwplot <- function(...) { print(lattice::bwplot(...)) }
histogram <- function(...) { print(lattice::histogram(...)) }
barchart <- function(...) { print(lattice::barchart(...)) }
require(vcd)
mosaic <- function(...) { print(vcd::mosaic(...)) }
@ 

\section{Simulation and Randomization with the  \texttt{mosaic} Package}

Software environments almost always provide an important feature:
\authNote{Why use itemize when you want to enumerate?  If we don't like the 
display of enumerate, we should reset the defaults.  I'm switching to enumerate.
---rjp}
\begin{enumerate}
  \item They allow potentially complicated operations to be
    packaged with a simple interface, making them easy to use.
	\saveenumi
\end{enumerate}
An environment that is integrated with a programming language provides
an additional capability:
\begin{enumerate}
\reuseenumi
  \item New operations can be constructed from existing ones.
\end{enumerate}

Any proper statistics environment should offer the ease of use of
(1).  \R, like many packages, offers a large set of pre-packaged
operations.  But, as a modern programming language, \R\ offers the
advantage of making (2) available as well as (1).  In addition to
enabling \R\ to provide ready access to new forms of state-of-the-art
computing, the programming-language features of \R\ also enable the
operations in (1) to be de-constructed and presented to students in a
transparent and intelligible way that reveals the underlying logic.

One of the important goals of the \verb!mosaic! package is to provide
elementary commands that can be easily strung together by novices
without having to master the esoteric aspects of programming.  This
chapter will describe a few such key operations and how they connect
to one another: random sampling and resampling, replication of random
trials, summarizing the results of multiple trials.  As you will see,
the \verb!mosaic! operations allow students to implement each of the
operations in what George Cobb calls the ``3 Rs'' of statistical
inference: Randomization, Replication, and Rejection. \cite{USCOTS-cobb-2005}
By putting the 3 Rs together in various ways, students learn to
generalize and internalize the logic of inference, rather than just
following formulaic methods.

\subsection{Sampling and Resampling}

Arguably, the most important operation in statistics is sampling:
ideally, selecting a random subset from a population.  Regrettably,
sampling takes work and time, so instructors tend to de-emphasize the
actual practice of sampling in favor of theoretical descriptions.
What's more, the algebraic notation in which much of conventional
textbook statistics is written does not offer an obvious notation for sampling.

With the computer, however, thes3 efficiency and notation obstacles
can be overcome.  Sampling can be placed in its rightfully central
place among the statistical concepts in our courses.

In doing so, why not start right off with a sample from a population?
One example is the population of US states:

\authNoted{DTK: I'm reading it in this awkward way until I can add the dataset to
  the \verb!mosaic! package.  In the \verb!mosaic! package, the current ``SAT'' data
set has just one variable.  My view is that all such data sets should
be data frames.  But I understand the virtue of having a few that
students can use without the \$ notation.}%
\authNote{RJP: I added the data and cleaned it up a bit.
All you need do is add documentation.}%
<<eval=false,echo=false>>=
SAT = read.csv("http://www.macalester.edu/~kaplan/ISM/datasets/sat.csv")
@ 

Students in the US will know that there are 50 states:
<<>>=
nrow(SAT)
@ 


\authNoted{DTK: Danny should fix the SAT data set so that states like
  ``New York'' don't have a comma inserted in place of the space.

  RJP:  done.}

One of the variables to be measured from the population is the
student/teacher ratio 
<<>>=
mean( SAT$ratio )
@ 

\authNote{DTK: There's another style that we could push.  I'm
  interested in your reaction to it. This has the advantage of segregating the
  randomization part from the calculation part.  
  I'll try using this style in the following, and we
can easily change back to a more conventional style if it gets irritating.}

<<>>=
with( SAT, mean(ratio) )
@ 

Every student who does this calculation will get the same result.
But if each student takes a random sample of, say, 4 states:
\authNote{RJP: If we want a sample, let's call it a sample, not a shuffle.  I've changed
the \R\ code throughout this section.}%
<<>>=
sample(SAT, 4)
@ 

Then each student gets a potentially different result.

Doing this with a classroom of students will immediately elicit a
realization that the random sample is exactly that.  Ask the class:
Who got California?  (There should be one or two in a class of 20.)
Who got Alabama?  Did anyone get the same state twice?
\authNote{RJP:
No one will get the same state twice with \verb!smaple()! or 
with \verb!shuffle()! under the default settings.  \verb!resample()! 
on the other hand, samples with replacement by default.
We could use \verb!deal()! here, but why mix metaphors when you are talking
about sampling?
}

\begin{problem}
QUESTION FOR INSTRUCTORS: How would you demonstrate that there are 50
different states, rather than just 50 cases that might include
duplicates?  (This isn't a question for beginning students, just one
for developing instructor capabilities.)  Try each of the following
and see if you can understand what's going on in each of them:
<<echo=false>>=
with( SAT, table(state) )
length(with( SAT, table(state) ))
table(with( SAT, table(state) ))
table(table(SAT$state))
@ 
END OF QUESTION FOR INSTRUCTORS 
\end{problem}

The value of some measure for the population is, of course, called the ``population
parameter.''  
\authNote{RJP: Why use scare quotes here?  Use \verb!\term{}! to highlight terms.  We 
can define that to be boldface font or some such.}%
But for a sample, it's a ``sample statistic.''  It's
worthwhile to make the distinction concrete:
<<>>=
with( SAT, mean(ratio) )
with( sample(SAT,4), mean(ratio) )
@ 

Good questions to ask in class at this point: Who got a sample
statistic that's bigger than the population parameter?  (About half
the class will say yes.)  Who got a sample statistic that's exactly
equal to the population parameter?  (It's unlikely that there will be a match.)
\authNote{Is it worth the effort to write a few macros that place marginal
notes alerting the reader to things like ``questions for students'', 
``just for instructors'', ``R pitfalls'', etc.  
Perhaps we could use the data set identifier from Danny's book
as a model?  I'm thinking it would be nice to make it easier for users to locate
various types of material.}


At this point, it's helpful to point out that the reason we take
samples is because it can be difficult or impossible to measure every
member of the population.  Of course, it's not so difficult if your
population is the 50 US States, but what if your population were the
whole set individual people living in the US: that's over 300,000,000
people!  


\subsection{Taking Randomness Seriously}

It's tempting to think that ``random'' means that anything goes.  This
is, after all, the everyday meaning of the word.  In statistics,
though, the point of randomness is to get a representative sample, or
to assign experimental treatments in some fairly even way.   
\authNote{RJP: 'fairly even' is pretty vague and potentially misleading.
Randomness is really about having some knowledge about the variability.
Other methods may have less variability, but if we don't know how to quantify it
and if it is miscentered (biased), it doesn't do us any good. }%
Using
randomness properly means taking care and being formal about where the
randomness comes from.  When you finally submit the research paper you
have written after your long years of labor, you don't want it to be
rejected because the reviewer isn't convinced that what you called
``random'' was really so.

First, to demonstrate that the sort of computer-generated random
sampling produces reasonable results.  Perhaps it's a little too early
in the course to talk about sampling distributions, but you certainly
can have your class generate random samples of a reasonable size and
show that sample statistics are reasonably close to the population parameter.  
\authNote{RJP: samples are not close to parameters, statistics are.  I changed
'they' to 'sample statistics'}%
For
instance, have each student in your class do this:
<<>>=
with( SAT, mean(ratio) )
with( sample(SAT,25), mean(ratio) )
@ 
Or, each student can repeat the trial several times:
<<>>=
do(10)*with( sample(SAT,25), mean(ratio) )
@ 
This raises the question, naturally enough, of what ``reasonably
close'' means, and what it means to measure ``how close.''
Eventually, that might lead you into consideration of differences,
mean differences, mean square differences, and root mean square
differences: the standard deviation and variance.  But even before
encountering this formality the
students should be able to see that there is nothing systematically
wrong with the answers they get from their random samples.  Some are too high compared to the
population parameter, some are too low, but as a group they are pretty
well centered.

\subsection{Illuminating Sampling Mistakes}
\authNote{RJP: The word 'illuminating' has two meanings intentionally -- but feel free to 
change if that's too cute.}%
It's helpful at this point to have some examples where an informal or careless
approach to randomness leads to misleading results.

Here are some that can be visually compelling:
\begin{enumerate}
\item Select books off a library shelf and count how many pages 
are in each book.
  When students pick books haphazardly, they get a systematic
  over-estimate of the page length.  
  
  A random sample of 25 is
  sufficient to see this for the large majority of students in the
  class.  Going up to 50, though tedious, makes the result even more compelling.
  \authNote{RJP:  I moved Danny's comment (Introduce the R app that let's
  students do this with the mouse.)  to this note.  
  It's too late in the game to put hard to spot cruft into the document
  that we need to remember to remove later.}%
  
\item A simulation of a real-world study of Alzheimer's disease.  The
  sampling process was to go to nursing homes on a given day and
  randomly pick out patients who were diagnosed with Alzheimers.
  Their files were flagged and, after a few years, the data were
  examined to find the mean survival time, from when they entered the
  nursing home to when they died.  
  
\authNote{We can easily write an app that generates admission and
  death dates as Julian dates, and let's the student pick a random day
  of the year for the sampling.  A graphic shows how the random day
  tends to select those who are at the nursing home for a long time,
  rather than equally picking all patients.  Students can then compare
  the mean for the population to the mean for their haphazard sample.}

  \item Sample people and find out how many siblings are in their family
  (including themselves).
  Use this data to estimate family size.  Since larger families have more
  children, we will over-sample larger families and over-estimate 
  family size.  (Section~\ref{sec:fake-families} demonstrates how
  to fake a data set for this activity.)

  (Note:  In a higher level course, you could ask students
  to determine a method to correct for this bias.)

  \authNote{RJP:  build appropriate data frames for this and Alzheimer's examples.
  Do we want to show how to build these?  I say yes.  It would be good for instructors
  to see how these are built.  Those who are not interested can simply
  copy.  We can but this stuff into a \verb!demo()! for easy access.
  }

  \item
  Using \verb!googleMap()! (see Section~\ref{sec:googleMap}) 
  we can compare uniform sampling of longitude 
  and latitude with correct sampling that takes into account that the earth
  is a sphere, not a cylinder.
\end{enumerate}

In each of these cases, follow up the haphazard or systematically biaseed 
sample with a formally generated sample (which is faster to do, in any event) and show that
the formally generated sample gives results that are more
representative of the population than the haphazardly sampled data.


\subsection{Sampling From Day 1 with \texttt{rflip()}}

Since tossing a coin is one of the most familiar examples of randomness, we've added to
\verb!mosaic! a function that facilitates simulating coin tosses. 
<<>>=
rflip(20)
as.numeric( rflip(20) )       # just count how many heads
@


In addition to the Lady Tasting Tea activity (Section~\ref{sec:lady-tasting-tea}),
other activities can be devised to help students develop a sense for randomness 
and variability.

\begin{enumerate}
\item If you flip a coin 100 times, how often will you get exactly 50 heads?
Between 45 and 55 heads?  Between 40 and 60 heads?
\item
If you flip a coin 20 times, how long is the longest run of either heads or tails typically?

This can be part of demonstration where you have students write down strings of H and T
that ``look random'' and another set based on actual coin tosses.  Then see if you can 
tell them which are which.

\item
\verb!rflip()! allows you to flip biased coins as well.  Let's simulate a 90\%
free throw shooter shooting 10 free throws.
\end{enumerate}
<<>>=
rflip(10, prob=.9)
@

\authNote{Add some more of these.  Should we include examples where the coin is 
biased -- free throw shooting comes to mind, for example.  Also the tipping 
and spinning pennies activities.}%

\subsection{Basic Sampling With \texttt{sample()}, \texttt{resample()}, and Cousins}

<<>>=
rdata(20, 1:4)              # samples with replacement
@

\authNote{\texttt{rdata()} could go in this section or in the next.  It serves as a 
transition from sampling from data to sampling from an abstract distribution.}%

\subsection{Sampling from Distributions}

%But better to sample from less abstract quantities and use these to
%generate some of the distributions that are used later.


<<>>=
rnorm(20, mean=500, sd=100) 
rexp(20, rate=1/5)
@

<<>>=
# These are equivalent
sample(cards, 5)
deal(cards, 5)
@
<<>>=
# These are equivalent
sample(cards) 
shuffle(cards) 
@

<<echo-false>>=
set.seed(123)
@
<<>>=
# These are equivalent.  Note the order of the arguments.
sample(HELP, 2)
rdata(2, HELP)
@

<<echo-false>>=
set.seed(123)
@
<<>>=
# These are equivalent; sampling with replacement now
resample(HELP, 2)
rdata(2, HELP, replace=TRUE)
@

\subsection{\texttt{do()}}
The heart of a simulation is doing something over and over.  
The \verb!do()! function simplifies the syntax for this and improves 
the output format (usually returning a data frame).

<<>>=
do(5) * "hello"
@

\subsection{Generating Fake Data For Sampling Activities}
\label{sec:fake-families}%
If you don't have actual data at your disposal, you can simulate data
to illustrate your point.  For example, let's take  the family size example above.
We'll simulate 5000 families using a Poisson distribution with a mean of 3
to generate family size.  (You can make up whatever elaborate story you like
for the population you are considering.)

<<>>=
families <- data.frame( familyid = 1:5000, children = rpois(5000,3) )
@

Now we generate the people in these families
<<>>=
people <- data.frame( 
		familyid = with( families, rep( familyid, children) ) ,
		sibs = with( families, rep( children, children) ) 
		)
@

<<>>=
with( families, mean(children) )
with( people, mean(sibs) )
@
If the result is mysterious, the following tables might shed some light.
<<>>=
with(families, table(children))
with(people, table(sibs))
@

Once built, you can provide these data frames to your students for a sampling exercise.
(See \ref{sec:distributingData}.)


\section{Empirical p-values}
\subsection{Lady Tasting Tea}
If you don't use the Lady Tasting Tea as a course starter, you can use it as 
an introduction to testing a proportion.

\subsection{Golfballs in the Yard}

<<>>=
golfballs <- c(137, 138, 107, 104) 
table( rdata(486, 1:4) )
@
<<>>=
rgolfballs <- do(2000) * table( rdata(486, 1:4) )
@


\begin{center}
<<golfballs-max,fig=true>>=
print( statTally( golfballs, rgolfballs, max ) )
@
\end{center}

\begin{center}
<<golfballs-range,fig=true,height=1.7>>=
print( statTally( golfballs, rgolfballs, function(x) { diff(range(x)) } ) )
@
\end{center}

\begin{center}
<<golfballs-var,fig=true,height=1.7>>=
print( statTally( golfballs, rgolfballs, var ) )
@
\end{center}


\subsection{Comparing Two Means}

In this section we show two ways to see if the mean ages of men and women in the HELP study 
are significantly different.

\subsubsection{Using \texttt{aggregate()}}

The \verb!aggregate()! function can compute a function on subsets of variable:
<<>>=
aggregate( age ~ sex, HELP, mean )
@
A natural test statistic for comparing two means is the difference in sample means,
which we can calculate using
<<>>=
diff(aggregate( age ~ sex, HELP, mean )$age)                    # actual
@

Using \verb!do()! and \verb!shuffle()! we can now compare
\Sexpr{round(diff(aggregate( age ~ sex, HELP, mean )$age),2)} with
the values we obtain if we randomly shuffle the \verb!sex! variable.

<<>>=
do(1) * diff(aggregate( age ~ sex, HELP, mean )$age)                      # actual
do(2) * diff(aggregate( age ~ shuffle(sex), HELP, mean )$age)             # shuffled
do(2) * c( diff=diff(aggregate( age ~ shuffle(sex), HELP, mean )$age) )   # shuffled and renamed
@


If we increase the number of random shufflings, we get an approximate sampling distribution
that we can use to compute an empirical p-value.

<<two-sample,width=4>>=
set.seed(123)
test.stat <- diff(aggregate( age ~ sex, HELP, mean )$age )
rtest.stats <- do(1000) * c(diff= diff(aggregate( age ~ shuffle(sex), HELP, mean )$age) )
rtest.stats <- rtest.stats$diff
@
<<two-sample-pval>>=
table(rtest.stats >= test.stat)
mean(rtest.stats >= test.stat)                      # compute proportion of extreme statistics
@
With an empirical p-value of approximately \Sexpr{round(mean(rtest.stats >= test.stat),3)}, 
there is no reason to reject the null hypothesis that the mean
age is the same for men and women.

We can use histograms or dotplots to display the sampling distributions graphically.
\begin{center}
<<two-sample-hist,fig=true,width=5>>=
xhistogram(~rtest.stats, v=test.stat, groups=rtest.stats >= test.stat)      
@
\end{center}

\begin{center}
<<two-sample-dot,fig=true,width=6,height=2.2>>=
test.stat <-  diff(aggregate( age ~ sex, HELP, mean )$age )
rtest.stats <- do(200) * c(diff= diff(aggregate( age ~ shuffle(sex), HELP, mean )$age) )
dotPlot(~diff, rtest.stats, n=40, groups=rtest.stats$diff >= test.stat, pch=16 )
ladd( panel.abline(v=test.stat) )                 # add a verticle line
@
\end{center}

\subsubsection{Using \texttt{lm()}}
An alternative approach to comparing two means is based on the use of linear models.  In the example below,
\R\ will automatically convert the \verb!sex! factor into 0's and 1's, so the slope parameter in the model
is the difference in the means.  
<<>>=
lm( age ~ sex, HELP )                        # actual data
@
The \verb!do()! function conveniently stores the values of the 
estimated parameters that result from fitting with \verb!lm()!, so it is relatively easy to obtain
the sampling distribution for any of these estimated parameters.
<<>>=
do(1) * lm( age ~ sex, HELP )                # actual data
do(2) * lm( age ~ shuffle(sex), HELP )       # shuffled data
@

Since this is just another way of computing the difference in the sample means, the two approaches
are equivalent.  If we reset the random seed, we get exactly the same simulation results.
<<>>=
set.seed(123)
simulated <- do(1000) * lm( age ~ shuffle(sex), HELP ) 
test.stat <- (do(1) * lm(age ~ sex, HELP) )$sexfemale -> test.stat
rtest.stats <- simulated$shuffle.sexfemale
table(rtest.stats >= test.stat)
mean(rtest.stats >= test.stat)
@
\authNote{Perhaps we should adjust do() so that it returns a data frame even when there is one repetition.
---rjp}
\authNote{Done. --rjp}


\section{Bootstrap Confidence Intervals}
\authNote{Should we interleave the confidence intervals and p-values or separate them like this? --rjp}%



\section{Exercises, Problems, and Activities}

\shipoutProblems

\section{Power}

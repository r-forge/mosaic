

\chapter{The Core of a Traditional Course}

\SweaveOpts{prefix.string=figures/fig}  % location of
\SweaveOpts{highlight=T}    % not sure this does anything unless we use pgfSweave
\SweaveOpts{tidy=F}         % keep.source probably disables this
\SweaveOpts{pdf=T}          % use pdf for graphics
\SweaveOpts{strip.white=T}  % remove blank lines at beginning and end
\SweaveOpts{keep.source=T}  % keeps formatting from original; allows ? to work
\SweaveOpts{width=5}
\SweaveOpts{height=2.2}

<<setup,echo=F>>=
source('setup.R')
@ 

In this chapter, we will briefly review the commands and functions needed
to analyze data from a more traditional introductory statistics course.
We will use data from the HELP study: a randomized trial of a novel 
way to link at-risk subjects with primary care.  More information on the
dataset can be found in section \ref{sec:help}.

Since the selection and order of topics can vary greatly from 
textbook to textbook and instructor to instructor, we have chosen to 
organize this material by the kind of data being analyzed.  This should make
it straightforward to find what you are looking for even if you present 
things in a different order.  This is also a good organizational template
to give your students to help them keep straight ``what to do when".

It's worth noting how we teach the ``Traditional Course'', which, contrary
to rumor, some of us do on occasion.  Nick has used the 5th edition of
Moore and McCabe \cite{moor:mcca:2006}, where he introduces least squares 
regression at the end of the first week of a semester, and multiple regression
(purely descriptively) at the end of the second week.  Design is the next
main topic, then a brutally pruned intro to probability and sampling distributions.
Interval estimation and testing are introduced in one or two settings, then 
the class returns to inference for multiple regression.  While students are working
on projects involving a multiple linear regression model with 2 predictors,
some categorical data analysis is covered.

\authNote{Danny and/or Randy: interested in chiming in?}

\section{One Quantitative Variable}

\subsection{Numerical Summaries}

R includes a number of commands to numerically summarize variables.
These include the capability of calculating the mean, standard deviation,
variance, median, five number summary, intra-quartile range (IQR) as well as arbitrary quantiles.  We will
illustrate these using the CESD (Center for Epidemiologic Studies - Depression)
measure of depressive symptoms.  To improve the legibility of output,
we will also set the default number of digits to display to a more reasonable
level (see \verb!?options! for more configuration possibilities).

<<cesd1,cache=F>>=
options(digits=3)
mean(HELP$cesd)
@
<<cesd2,cache=F>>=
sd(HELP$cesd)
@
<<cesd2b,cache=F>>=
sd(HELP$cesd)^2; var(HELP$cesd)
@

It is also straightforward to calculate quantiles of the distribution.

<<cesd3>>=
median(HELP$cesd)
five <- fivenum(HELP$cesd)
five     # display the object (vector of length 5)
five[4] - five[2]
IQR(HELP$cesd)
@

The user can interact with the objects created by these
functions to calculate the IQR (or the built-in a function can generate this directly).

<<cesd4>>=
quantile(HELP$cesd)   
quantile(HELP$cesd, c(.25, .50))
favstats(HELP$cesd)
@

By default, the 
\verb!quantile()! function displays the quartiles, but can be given
a vector of quantiles to display.  Finally, the \verb!favstats()! 
function in the \verb!mosaic! package provides a concise summary of 
many useful statistics.


\subsection{Graphical Summaries}
The \verb!histogram()! function is used to create a histogram.

\vspace{-4mm}
\begin{center}
<<cesd-hist,fig=TRUE>>=
histogram(~ cesd, HELP)
@
\end{center}

In the HELP dataset, approximately one quarter of the subjects
are female.  It is straightforward to restrict our attention to
just those subjects.

The \verb!subset()! function can generate a new data frame containing
just the women or just the men.  Once this is created, we
used the \verb!stem()! function to create a stem and leaf plot.
<<cesd-stem>>=
female <- subset(HELP, sex=='female')
male <- subset(HELP, sex=='male')
stem(female$cesd)
@

The \verb!dotPlot()! function is used to create a dotplot (a la Fathom).

\begin{center}
<<cesd-dot,fig=TRUE,height=1.8>>=
dotPlot(~ cesd, data=female)
@
\end{center}
Here we use the formula interface as discussed in section \ref{sec:formula} to
specify that we wanted a histogram of the CESD scores.

We could also have made our subset ``on the fly'', just for the purposes of graphing:
\begin{center}
<<cesd-dot2,fig=TRUE,height=1.8>>=
dotPlot(~ cesd, data=HELP, subset=(sex=='female'))
@
\end{center}

Or we could make side-by-side dotplots of men and women:
\begin{center}
<<cesd-dot3,fig=TRUE>>=
dotPlot(~ cesd | sex, data=HELP)
@
\end{center}


\subsection{Density Curves }

One disadvantage of histograms is that they can be sensitive to the choice of the
number of bins.  Another display to consider is a density curve.

\begin{center}
<<dens1,fig=TRUE>>=
densityplot(~ cesd, data=female)
ladd(grid.text(x=0.2, y=0.8, 'only females'))
@
\end{center}

\authNote{NH: I would like to add superimposed density plots.  Is this feasible given
that we need to protect "densityplot" with a print to deal with Weave?}

The most famous density curve is a normal distribution.  The \verb!xpnorm()! function
displays the probability that a random variable is less than the first argument, for a 
normal distribution with mean given by the second argument and standard deviation by the 
third.  More information about probability distributions can 
be found in section \ref{sec:probability}.

\subsection{Normal Distributions}
\begin{center}
<<norm1,fig=TRUE,height=2.4>>=
xpnorm(1.96, 0, 1)
@
\end{center}

\authNote{NH: I would like to add superimposed normal on top of a density plots.  Help!}




\section{One Categorical Variable}

\subsection{Numerical Summaries}

The \verb!xtabs()!, \verb!perctable()! and \verb!proptable()! functions can be used to calculate
counts, percentages and proportions, respectively for a categorical variable.

<<homeless-table>>=
xtabs(~ homeless, HELP)      # display table using counts
perctable(HELP$homeless)     # display table using percentages
proptable(HELP$homeless)     # display table using proportions
@
\authNote{NH: how best to add row and/or column totals?}

\subsection{The Binomial Test}

An exact confidence interval for a proportion (as well as a test of the null 
hypothesis is equal to a particular value [by default 0.5]) can be calculated
using the \verb!binom.test()! function.

<<binomtest>>=
binom.test(~ homeless=="housed", HELP)
@

\subsection{The Proportion Test}

A similar interval and test can be calculated using \verb!prop.test()!.
<<proptest>>=
prop.test(xtabs(~ homeless, HELP), correct=FALSE)
@
\authNote{suggestions for a better approach?}

\subsection{Goodness of Fit Tests}

A variety of goodness of fit tests can be calculated against a reference 
distribution.  For the HELP data, we could test the null hypothesis that the 
proportion of subjects back in the original population in each of the 
substance abuse groups were equal.

<<gof>>=
substab <- xtabs(~ substance, HELP)
substab
@
<<gofmore>>=
perctable(HELP$substance)
p <- c(1/3, 1/3, 1/3)
chisq.test(substab, p=p)
total <- sum(substab)
@

We can also calculate this quantity manually, in terms of observed and expected values.

<<gof2>>=
chisq = sum((substab - total*p)^2/(total*p))
chisq
1 - pchisq(chisq, 2)
rm(substab, p, total, chisq)
@


\section{Two Quantitative Variables}

\subsection{Paired t}

\verb!paired=TRUE!

\subsection{Scatterplots}

\begin{center}
<<xyplot,fig=TRUE>>=
xyplot(cesd ~ mcs, data=HELP)
ladd(panel.loess(HELP$mcs, HELP$cesd))
@
\end{center}

\subsection{Correlation}
<<corr>>=
with(HELP, cor(cesd, mcs))
@

\subsection{Simple Linear Regression}

<<lm>>=
model <- lm(cesd ~ mcs, data=HELP)
summary(model)
@

\begin{center}
<<lmhist,fig=TRUE>>=
histogram(~ residuals(model))
@
\end{center}

\begin{center}
<<lmplot,fig=TRUE>>=
plot(model)
@
\end{center}

\section{Two Categorical Variables}

\subsection{Cross classification Tables}

\subsubsection{From Raw Data}
<<homeless-sex>>=
xtabs(~ homeless + sex, HELP)     
@

\subsubsection{Building Cross Tables from Already Summarized Data}

\begin{verbatim}
> mytab <- enterxtab()
Row variable name: 
1: sex
Read 1 item
Row variable levels: 
1: male
2: female
3:                 # entered a blank line here
Read 2 items
Column variable name: 
1: status
Read 1 item
Column variable levels: 
1: homeless
2: housed
3:                 # and here
Read 2 items
Row 1: 
1: 169 40
Read 2 items
Row 2: 
1: 177 67
Read 2 items
> mytab
        status
sex      homeless housed
  male        169     40
  female      177     67
\end{verbatim}

\authNote{Suggestions about how to format this?}

\subsubsection{Graphical Summaries}

\begin{center}
<<mosaicplot,fig=TRUE>>=
with(HELP, mosaicplot(table(homeless, sex)))
@
\end{center}

\subsection{Chi-squared tests}

<<chisq1>>=
with(HELP, chisq.test(homeless, sex, correct=FALSE))
@

\subsubsection{Displaying additional information}

<<chisq2>>=
with(HELP, xchisq.test(homeless, sex, correct=FALSE))
@

\subsection{Fisher's Exact Test}

<<help-fisher>>=
with(HELP, fisher.test(homeless, sex))
@

\section{Quantitative Response to a Categorical Predictor}

\subsection{A Bivariate Predictor: Two-sample t}
Here we will compare the distributions of CESD scores by sex.
The \verb!HELP! data set has a numeric variable \verb!female!
that codes sex us 0 for male and 1 for female.
To ensure that the display is clearly
labeled, we first create a new factor (i.e., categorical) variable called 
\verb!gender!.

<<cesd-gender-better>>=
HELP$gender <- factor(HELP$female, labels=c('male', 'female'))
# sanity check:
xtabs( ~ female + gender, HELP)
@
As it turns out, the \verb!HELP! data set already had a recoded 
version of female called \verb!sex!.  But many data sets require this sort
of conversion from numeric codes to factors.
<<>>=
# HELP already has a factor called sex, so this was redundant effort
xtabs( ~ sex + gender, HELP)
@

The \verb!tapply()! function can be used to calculate the mean CESD score
(given as first argument)
for each of the two groups (given as second argument).  The third argument
specifies which function to run for each of these subgroups.
<<tapply>>=
aggregate(cesd ~ sex, HELP, FUN=mean)
aggregate(cesd ~ sex, HELP, FUN=favstats)
@

Boxplots are a particularly helpful graphical display to compare distributions.
The \verb!bwplot()! function can be used to display the boxplots for the CESD scores separately by sex.  We see from both the numerical and graphical
displays that women tend to have slightly higher CESD scores than men.
\authNote{Should mention use of \verb!xyplot()! when the sample sizes are small.
No reason to compute 5-number summaries of 10 or fewer numbers really.  NH resp: I'm not sure
what you mean by this.}

%\vspace{-8mm}
\begin{center}
<<cesd-box,fig=TRUE>>=
bwplot(cesd ~ sex, data=HELP)
@
\end{center}

\subsection{Nonparametric 2 group tests}

<<help-nonpar>>=
wilcox.test(cesd ~ sex, data=HELP)
@

\subsection{Oneway ANOVA}
\begin{center}
<<cesd-oneway,fig=TRUE,width=6,height=1.9>>=
bwplot(cesd ~ substance, data=HELP)
@
\end{center}
<<help-aov>>=
mod <- aov(cesd ~ substance, data=HELP)
summary(mod)
@
<<help-aovlm>>=
mod1 <- lm(cesd ~ 1, data=HELP)
mod2 <- lm(cesd ~ substance, data=HELP)
anova(mod1, mod2)
@


\subsection{Tukey's Honest Significant Differences}

<<help-hsd>>=
with(HELP, tapply(cesd, substance, mean))
TukeyHSD(mod, "substance")
@

\section{Categorical Response to a Quantitative Predictor}

\subsection{Logistic Regression}

<<help-logit>>=
summary(glm(homeless ~ age + female, binomial, data=HELP))
@

\section{Survival Time Outcomes}

\subsection{Kaplan-Meier plot}

\begin{center}
<<help-km, fig=TRUE,width=6,height=3.9>>=
library(survival)
fit <- survfit(Surv(dayslink, linkstatus) ~ treat, data=HELP)
plot(fit, conf.int=FALSE, lty=1:2, lwd=2, xlab="time (in days)",
  ylab="P(not linked)")
legend(20, 0.4, legend=c("Control", "Treatment"), lty=c(1,2),
  lwd=2)
title("Product-Limit Survival Estimates (time to linkage)")
@
\end{center}

\subsection{Cox proportional hazards model}

<<help-surv>>=
library(survival)
summary(coxph(Surv(dayslink, linkstatus) ~ age + substance, data=HELP))
@


\section{More than Two Variables}

\subsection{Two-way ANOVA}

<<help-aov2>>=
summary(aov(cesd ~ homeless + sex, data=HELP))
@


\subsection{Multiple Regression}

<<help-multreg>>=
lm1 <- lm(cesd ~ mcs + age + sex, data=HELP)
summary(lm1)
@
<<help-multreg2>>=
lm2 <- lm(cesd ~ mcs + age + sex + mcs*sex, data=HELP)
summary(lm2)
@
<<help-multreg3>>=
anova(lm1, lm2)
@

\section{Probability and Random Variables}

\label{sec:DiscreteDistributions}
\label{sec:probability}

R can calculate quantities related to probability distributions of all types.  
It is straightforward to generate
random variables from these distributions, which can be used 
for simulation and analysis.

Table \ref{core:dist} displays the basenames for probability distributions 
available within base R.

XX ADD info on usage.

\begin{table}
\begin{center}
\caption{basename for probability distribution and random number generation}
\label{core:dist}
\begin{tabular}{|c|c|c|} \hline
Distribution   & NAME                 \\ \hline
Beta           &  {\tt beta}        \\
binomial       &  {\tt binom}    \\
Cauchy         &  {\tt cauchy}   \\
chi-square     &  {\tt chisq}    \\
exponential    &  {\tt exp}      \\
F              &  {\tt f}        \\
gamma          &  {\tt gamma}    \\
geometric      &  {\tt geom}     \\
hypergeometric &  {\tt hyper}    \\
logistic       &  {\tt logis}    \\
lognormal      &  {\tt lnorm}    \\
negative binomial &  {\tt nbinom} \\
normal         &  {\tt norm}      \\
Poisson        &  {\tt pois}      \\
Student's t    &  {\tt t}        \\
Uniform        &  {\tt unif}     \\
Weibull        &  {\tt weibull}   \\ \hline
\end{tabular}
\end{center}
\end{table}



\section{Power Calculations}

\label{sec:onesamppower}

While not generally a major topic in introductory courses, power and sample size calculations
help to reinforce key ideas in statistics.  In this section, we will explore how R
can be used to undertake power calculations using analytic approaches (see \ref{sec:power}
for simulation based approaches).  We consider a simple problem with two tests (t-test and
sign test) of
a one-sided comparison.

Let $X_1, ..., X_{25}$ be i.i.d. $N(0.3, 1)$.  Consider testing the null hypothesis $H_0: \mu=0$ versus $H_A: \mu>0$ at significance level $\alpha=.05$.  Compare the power of the sign test and the power of the test based on normal theory (one sample one sided t-test) assuming that $\sigma$ 
is known.

\subsection{Sign test}

We first start by calculating the Type-I error rate for the sign test.  Here we want to
reject when the number of positive values is large.  Under the null hypothesis, this is
distributed as a Binomial random variable with n=25 trials and p=0.5 probability of being
a positive value.  Let's consider values between 15 and 19.
<<pbinom>>=
xvals <- 15:19
probs <- 1 - pbinom(xvals, 25, 0.5)
cbind(xvals, probs)
@
So we see that if we decide to reject when X = the number of positive values is
17 or larger, we will have an $\alpha$ level of \Sexpr{round(1-pbinom(16, 25, 0.5), 3)},
which is near the nominal value in the problem.

We calculate the power of the sign test as follows. The probability that $X > 0$, given that $H_A$ is true is given by:
<<pnorm1>>=
1-pnorm(0, 0.3, 1)
@
We can view this graphically using the command:

\begin{center}
<<pnorm2,fig=TRUE,width=6,height=1.9>>= 
xpnorm(0, 0.3, 1)
@
\end{center}

So now we need to find the power under the alternative, which is equal to the probability of getting 17 or more positive values,
given that $p=0.6179$:
<<pbinom2>>=
1 - pbinom(16, 25, 0.6179)
@

The power is modest at best.

\subsection{T-test}

We next calculate the power of the test based on normal theory.  To keep the comparison
fair, we will set our $\alpha$ level equal to 0.05388.
First we need to find the rejection region.  

<<>>=
alpha <- 1-pbinom(16, 25, .5) # = 0.0539, our alpha level.
n <- 25; sigma <- 1 # given
stderr <- sqrt(sigma^2/n)
zstar <- qnorm(1-alpha, 0, 1)
zstar
crit <- zstar*stderr
crit
@
Therefore, we reject for observed means greater than \Sexpr{round(crit,3)}.  

To calculate the power of this one-sided test we find the area 
under the alternative hypothesis 
that is to the right of this cutoff:
<<>>=
power <- 1 - pnorm(crit, 0.3, stderr)
power
@
Thus, the power of the test based on normal theory is \Sexpr{round(power,3)}.
To provide a general check (or for future calculations of this sort) we can use the {\tt power.t.test()} in R.
<<>>=
power.t.test(n=25, delta=.3, sd=1, sig.level=alpha, alternative="one.sided",
type="one.sample")$power
@

This yields a similar estimate to the value that we calculated directly.  
Overall, we see that the t-test has higher power than the sign test, if the underlying
data are truly normal.  It's useful to have students calculate power empirically, 
as demonstrated for this example in \ref{sec:power}.




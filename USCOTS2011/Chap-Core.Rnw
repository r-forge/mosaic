

\chapter{The Core of a Traditional Course}

\SweaveOpts{prefix.string=figures/fig}  % location of
\SweaveOpts{highlight=T}    % not sure this does anything unless we use pgfSweave
\SweaveOpts{tidy=F}         % keep.source probably disables this
\SweaveOpts{pdf=T}          % use pdf for graphics
\SweaveOpts{strip.white=T}  % remove blank lines at beginning and end
\SweaveOpts{keep.source=T}  % keeps formatting from original; allows ? to work
\SweaveOpts{width=5}
\SweaveOpts{height=2.2}

<<setup,echo=F>>=
source('setup.R')
@ 

In this chapter, we will briefly review the commands and functions needed
to analyze data from a more traditional introductory statistics course.
We will use data from the HELP study: a randomized trial of a novel 
way to link at-risk subjects with primary care.  More information on the
dataset can be found in section \ref{sec:help}.

Since the selection and order of topics can vary greatly from 
textbook to textbook and instructor to instructor, we have chosen to 
organize this material by the kind of data being analyzed.  This should make
it straightforward to find what you are looking for even if you present 
things in a different order.  This is also a good organizational template
to give your students to help them keep straight ``what to do when".

It's worth noting how we teach the ``Traditional Course'', which, contrary
to rumor, some of us do on occasion.  Nick has used the 5th edition of
Moore and McCabe \cite{moor:mcca:2006}, where he introduces least squares 
regression at the end of the first week of a semester, and multiple regression
(purely descriptively) at the end of the second week.  Design is the next
main topic, then a brutally pruned intro to probability and sampling distributions.
Interval estimation and testing are introduced in one or two settings, then 
the class returns to inference for multiple regression.  While students are working
on projects involving a multiple linear regression model with 2 predictors,
some categorical data analysis is covered.

\authNote{Danny and/or Randy: interested in chiming in?}

\section{One Quantitative Variable}

\subsection{Numerical Summaries}

\R includes a number of commands to numerically summarize variables.
These include the capability of calculating the mean, standard deviation,
variance, median, five number summary, intra-quartile range (IQR) as well as arbitrary quantiles.  We will
illustrate these using the CESD (Center for Epidemiologic Studies - Depression)
measure of depressive symptoms.  To improve the legibility of output,
we will also set the default number of digits to display to a more reasonable
level (see \verb!?options! for more configuration possibilities).

<<cesd1,cache=F>>=
options(digits=3)
mean(HELP$cesd)
@
<<cesd2,cache=F>>=
sd(HELP$cesd)
@
<<cesd2b,cache=F>>=
sd(HELP$cesd)^2; var(HELP$cesd)
@

It is also straightforward to calculate quantiles of the distribution.

<<cesd3>>=
median(HELP$cesd)
five <- fivenum(HELP$cesd)
five     # display the object (vector of length 5)
five[4] - five[2]
IQR(HELP$cesd)
@

The user can interact with the objects created by these
functions to calculate the IQR (or the built-in a function can generate this directly).

<<cesd4>>=
quantile(HELP$cesd)   
quantile(HELP$cesd, c(.25, .50))
favstats(HELP$cesd)
@

By default, the 
\VN{quantile()} function displays the quartiles, but can be given
a vector of quantiles to display.  Finally, the \VN{favstats()}
function in the \verb!mosaic! package provides a concise summary of 
many useful statistics.


\subsection{Graphical Summaries}
The \VN{histogram()} function is used to create a histogram.

\vspace{-4mm}
\begin{center}
<<cesd-hist,fig=TRUE>>=
histogram(~ cesd, HELP)
@
\end{center}

In the HELP dataset, approximately one quarter of the subjects
are female.  It is straightforward to restrict our attention to
just those subjects.

The \VN{subset()} function can generate a new data frame containing
just the women or just the men.  Once this is created, we
used the \VN{stem()} function to create a stem and leaf plot.
<<cesd-stem>>=
female <- subset(HELP, sex=='female')
male <- subset(HELP, sex=='male')
stem(female$cesd)
@

The \VN{dotPlot()} function is used to create a dotplot (a la Fathom).

\begin{center}
<<cesd-dot,fig=TRUE,height=1.8>>=
dotPlot(~ cesd, data=female)
@
\end{center}
Here we use the formula interface (as discussed in section \ref{sec:formula}) to
specify that we wanted a histogram of the CESD scores.

We could also have made our subset ``on the fly'', just for the purposes of graphing:
\begin{center}
<<cesd-dot2,fig=TRUE,height=1.8>>=
dotPlot(~ cesd, data=HELP, subset=(sex=='female'))
@
\end{center}

Or we could make side-by-side dotplots of men and women:
\begin{center}
<<cesd-dot3,fig=TRUE>>=
dotPlot(~ cesd | sex, data=HELP)
@
\end{center}


\subsection{Density Curves }

One disadvantage of histograms is that they can be sensitive to the choice of the
number of bins.  Another display to consider is a density curve.

\begin{center}
<<dens1,fig=TRUE>>=
densityplot(~ cesd, data=female)
ladd(grid.text(x=0.2, y=0.8, 'only females'))
@
\end{center}

\authNote{NH: I would like to add superimposed density plots.  Is this feasible given
that we need to protect "densityplot" with a print to deal with Weave?}

The most famous density curve is a normal distribution.  The \VN{xpnorm()} function
displays the probability that a random variable is less than the first argument, for a 
normal distribution with mean given by the second argument and standard deviation by the 
third.  More information about probability distributions can 
be found in section \ref{sec:probability}.

\subsection{Normal Distributions}
\begin{center}
<<norm1,fig=TRUE,height=2.4>>=
xpnorm(1.96, 0, 1)
@
\end{center}

\authNote{NH: I would like to add superimposed normal on top of a density plots.  Help!}




\section{One Categorical Variable}

\subsection{Numerical Summaries}

The \VN{xtabs()}, \VN{perctable()} and \VN{proptable()} functions can be used to calculate
counts, percentages and proportions, respectively for a categorical variable.

<<homeless-table>>=
xtabs(~ homeless, HELP)      # display table using counts
perctable(HELP$homeless)     # display table using percentages
proptable(HELP$homeless)     # display table using proportions
@
\authNote{NH: how best to add row and/or column totals?}

\subsection{The Binomial Test}

An exact confidence interval for a proportion (as well as a test of the null 
hypothesis is equal to a particular value [by default 0.5]) can be calculated
using the \VN{binom.test()} function.

<<binomtest>>=
results = binom.test(~ homeless=="housed", HELP)
results
names(results)
@

\authNote{NH: this is pretty strange (I would have thought that this would have estimated
p=0.539)}

As is generally the case with commands of this sort, there are a number of useful quantities available from 
the \verb!results! object.  For example, the user can extract the confidence interval.
<<binomtest2>>=
results$conf.int
results$conf.int[1]
@

\subsection{The Proportion Test}

A similar interval and test can be calculated using \VN{prop.test()}.
<<proptest>>=
prop.test(xtabs(~ homeless, HELP), correct=FALSE)
@
\authNote{suggestions for a better approach?}

\subsection{Goodness of Fit Tests}

A variety of goodness of fit tests can be calculated against a reference 
distribution.  For the HELP data, we could test the null hypothesis that the 
proportion of subjects back in the original population in each of the 
substance abuse groups were equal.

<<gof>>=
substab <- xtabs(~ substance, HELP)
substab
@
<<gofmore>>=
perctable(HELP$substance)
p <- c(1/3, 1/3, 1/3)
chisq.test(substab, p=p)
total <- sum(substab); total
total*p
@

We can also calculate this quantity manually, in terms of observed and expected values.

<<gof2>>=
chisq <- sum((substab - total*p)^2/(total*p)); chisq
1 - pchisq(chisq, 2)
rm(substab, p, total, chisq)
@


\section{Two Quantitative Variables}

\subsection{Scatterplots}

We always encourage students to start any analysis by graphing their data.  Here we augment a scatterplot
of the CESD (depressive symptoms) and the MCS (mental component score from the SF-36)
with a lowess (locally weighted scatterplot smoother) line.  

\begin{center}
<<xyplot,fig=TRUE,height=3>>=
xyplot(cesd ~ mcs, data=HELP)
ladd(panel.loess(HELP$mcs, HELP$cesd))
@
\end{center}

\subsection{Correlation}

Correlations can be calculated for a pair of variables, or for a matrix of variables.
<<corr>>=
with(HELP, cor(cesd, mcs))
with(HELP, cor(cbind(cesd, mcs, pcs)))
@

By default, Pearson correlations are provided, other variants can be specified using the
\verb!method! option.
<<corrspear>>=
with(HELP, cor(cesd, mcs), method="spearman")
@

\subsection{Simple Linear Regression}

Linear regression models were introduced previously in 
section \ref{sec:lm}.  These use the same formula interface to specify the outcome 
and predictors.


<<lm>>=
model <- lm(cesd ~ mcs, data=HELP)
coef(model)
@
<<lmsum>>=
summary(model)
@


<<lmclass>>=
class(model)
@
The return value from \VN{lm()} is a linear model object;
A number of functions can operate on these objects, as
seen previously with \VN{coef()}.  As an additional example,
there is a function called \VN{residuals()} that will return a
vector of
the residuals.


\begin{center}
<<lmhist,fig=TRUE>>=
histogram(~ residuals(model))
@
\end{center}
In addition, there are 
default methods 
to generate plots of
linear regression objects
(see \verb!?plot.lm!).
\begin{center}
<<lmplot,fig=TRUE,height=4>>=
plot(model)
@
\end{center}

\section{Two Categorical Variables}


\subsection{Cross classification Tables}

Cross classification (two-way or $R$ by $C$) tables can be constructed for
two (or more) categorical variables.  Here we consider the contingency table
for homeless status (homeless one or more nights in the past 6 months or housed) 
and sex.

\subsubsection{From Raw Data}
<<homeless-sex>>=
xtabs(~ homeless + sex, HELP)     
@

\subsubsection{Building Cross Tables from Already Summarized Data}

It can be handy to create a table from existing summaries.  Here 
we demonstrate how to generate this for our table of homeless status
by sex.

\begin{verbatim}
> mytab <- enterxtab()
Row variable name: 
1: sex
Read 1 item
Row variable levels: 
1: male
2: female
3:                 # entered a blank line here
Read 2 items
Column variable name: 
1: status
Read 1 item
Column variable levels: 
1: homeless
2: housed
3:                 # and here
Read 2 items
Row 1: 
1: 169 40
Read 2 items
Row 2: 
1: 177 67
Read 2 items
> mytab
        status
sex      homeless housed
  male        169     40
  female      177     67
\end{verbatim}

\authNote{Suggestions about how to format this?}

\subsubsection{Graphical Summaries}

Graphical summaries of cross classification tables may be helpful in visualizing
associations.  Mosaic plots are one example.  Here we see that males tend to be over-represented
amongst the homeless subjects (as represented by the horizontal line which is higher for
the homeless rather than the housed).  


\begin{center}
<<mosaicplot,fig=TRUE,height=3.5>>=
with(HELP, mosaicplot(table(homeless, sex)))
@
\end{center}

\subsection{Chi-squared tests}

<<chisq1>>=
with(HELP, chisq.test(homeless, sex, correct=FALSE))
@

There is a statistically significant association found: it is unlikely that we would observe
an association this strong if there was homeless status and sex were independent back in the 
population.

\subsubsection{Displaying additional information}

When a student finds a significant association, 
it's important for them to be able to interpret this in the context of the problem. 
The \VN{xchisq.test()} function provides additional details to help with this process.

<<chisq2>>=
with(HELP, xchisq.test(homeless, sex, correct=FALSE))
@

We observe that there are fewer homeless women, and more homeless men that would be expected.

\subsection{Fisher's Exact Test}

An exact test can also be calculated.  This is fairly computationally straightforward for 2 by 2
tables.  Options to help constrain the size of the problem for larger tables exist
(see \verb!?fisher.test()!).

<<help-fisher>>=
with(HELP, fisher.test(homeless, sex))
@

\section{Quantitative Response to a Categorical Predictor}

\subsection{A Bivariate Predictor: Numerical and Graphical Summaries}
Here we will compare the distributions of CESD scores by sex.

The \VN{aggregate()} function can be used to calculate the mean CESD score
for each of the two groups 
<<aggregate>>=
aggregate(cesd ~ sex, HELP, FUN=mean)
aggregate(cesd ~ sex, HELP, FUN=favstats)
@

Boxplots are a particularly helpful graphical display to compare distributions.
The \VN{bwplot()} function can be used to display the boxplots for the CESD scores separately by sex.  We see from both the numerical and graphical
displays that women tend to have slightly higher CESD scores than men.
\authNote{Should mention use of \VN{xyplot()} when the sample sizes are small.
No reason to compute 5-number summaries of 10 or fewer numbers really.  NH resp: I'm not sure
what you mean by this.}

%\vspace{-8mm}
\begin{center}
<<cesd-box,fig=TRUE>>=
bwplot(cesd ~ sex, data=HELP)
@
\end{center}

\subsection{A Bivariate Predictor: Two-sample t}

The Student's two sample t-test can be run without or with an equal variance assumption.

<<help-nonpar>>=
t.test(cesd ~ sex, data=HELP, var.equal=FALSE)
@

We see that there is a statistically significant difference between the two groups.


\subsection{Non-parametric 2 group tests}

The same conclusion is seen using a non-parametric (Wilcoxon rank sum) test.

<<help-nonpar>>=
wilcox.test(cesd ~ sex, data=HELP)
@

\subsection{Non-parametric 2 group tests}



\subsection{Permutation test}

Here we extend the methods introduced in section \ref{sec:comparing-two-means} to 
undertake a two-sided test.

<<permute,fig=TRUE>>=
simulations <- 1000
test.stat <-  abs(diff(aggregate(cesd ~ sex, HELP, mean)$cesd))
test.stat
rtest.stats <- do(simulations) * c(diff= diff(aggregate(cesd ~ shuffle(sex), HELP, mean)$cesd))
rtest.stats[simulations+1,] = test.stat  # add in the observed value
dotPlot(~ abs(diff), rtest.stats, n=40, groups=abs(rtest.stats$diff) >= test.stat, pch=16, cex=.8 )
ladd(panel.abline(v=test.stat))                 # add a vertical line
@

\authNote{NH: comments and suggestions welcomed: this is a kludge}

The same conclusion is observed with a permutation test.

\subsection{Oneway ANOVA}

Earlier comparisons were between two groups: we can also consider testing differences 
in CESD scores by primary substance of abuse (heroin, cocaine, or alcohol).

\begin{center}
<<cesd-oneway,fig=TRUE,width=6,height=1.9>>=
bwplot(cesd ~ substance, data=HELP)
@
\end{center}

<<aggregate2>>=
aggregate(cesd ~ substance, HELP, FUN=mean)
<<help-aov>>=
mod <- aov(cesd ~ substance, data=HELP)
summary(mod)
@
<<help-aovlm>>=
mod1 <- lm(cesd ~ 1, data=HELP)
mod2 <- lm(cesd ~ substance, data=HELP)
@

The \VN{anova()} command can be used to compare two (nested) models.

<<help-aov2>>=
anova(mod1, mod2)
@


\subsection{Tukey's Honest Significant Differences}

There are a variety of multiple comparison procedures.  One of these is Tukey's Honest
Significant Difference (HSD).  Other options are available with the \verb!multcomp! package.

<<help-hsd>>=
with(HELP, tapply(cesd, substance, mean))
@
<<help-hsd2>>=
TukeyHSD(mod, "substance")
@

\section{Categorical Response to a Quantitative Predictor}

\subsection{Logistic Regression}

Logistic regression is available from within the \VN{glm()} function: a variety of
link functions and distributional forms for generalized linear models are supported.

<<help-logit>>=
summary(glm(homeless ~ age + female, binomial, data=HELP))
@

\section{Survival Time Outcomes}

Extensive support for Survival (Time to Event) analysis is available within the 
\verb!survival! package.

\subsection{Kaplan-Meier plot}

\begin{center}
<<help-km, fig=TRUE,width=6,height=3.9>>=
library(survival)
fit <- survfit(Surv(dayslink, linkstatus) ~ treat, data=HELP)
plot(fit, conf.int=FALSE, lty=1:2, lwd=2, xlab="time (in days)",
  ylab="P(not linked)")
legend(20, 0.4, legend=c("Control", "Treatment"), lty=c(1,2),
  lwd=2)
title("Product-Limit Survival Estimates (time to linkage)")
@
\end{center}

We see that the subjects in the treatment (HELP clinic) were significantly more likely to 
link to primary care than the control (usual care) group.

\subsection{Cox proportional hazards model}

<<help-surv>>=
library(survival)
summary(coxph(Surv(dayslink, linkstatus) ~ age + substance, data=HELP))
@

Neither age, nor substance group was significantly associated with linkage to primary care.


\section{More than Two Variables}

\subsection{Two-way ANOVA}

<<help-aov2>>=
summary(aov(cesd ~ homeless + sex, data=HELP))
@


\subsection{Multiple Regression}

<<help-multreg>>=
lm1 <- lm(cesd ~ mcs + age + sex, data=HELP)
summary(lm1)
@
<<help-multreg2>>=
lm2 <- lm(cesd ~ mcs + age + sex + mcs*sex, data=HELP)
summary(lm2)
@
<<help-multreg3>>=
anova(lm1, lm2)
@

\section{Probability and Random Variables}

\label{sec:DiscreteDistributions}
\label{sec:probability}

\R can calculate quantities related to probability distributions of all types.  
It is straightforward to generate
random variables from these distributions, which can be used 
for simulation and analysis.

Table \ref{core:dist} displays the basenames for probability distributions 
available within base \R.

XX ADD info on usage.

\begin{table}
\begin{center}
\caption{basename for probability distribution and random number generation}
\label{core:dist}
\begin{tabular}{|c|c|c|} \hline
Distribution   & NAME                 \\ \hline
Beta           &  {\tt beta}        \\
binomial       &  {\tt binom}    \\
Cauchy         &  {\tt cauchy}   \\
chi-square     &  {\tt chisq}    \\
exponential    &  {\tt exp}      \\
F              &  {\tt f}        \\
gamma          &  {\tt gamma}    \\
geometric      &  {\tt geom}     \\
hypergeometric &  {\tt hyper}    \\
logistic       &  {\tt logis}    \\
lognormal      &  {\tt lnorm}    \\
negative binomial &  {\tt nbinom} \\
normal         &  {\tt norm}      \\
Poisson        &  {\tt pois}      \\
Student's t    &  {\tt t}        \\
Uniform        &  {\tt unif}     \\
Weibull        &  {\tt weibull}   \\ \hline
\end{tabular}
\end{center}
\end{table}



\section{Power Calculations}

\label{sec:onesamppower}

While not generally a major topic in introductory courses, power and sample size calculations
help to reinforce key ideas in statistics.  In this section, we will explore how \R
can be used to undertake power calculations using analytic approaches (see \ref{sec:power}
for simulation based approaches).  We consider a simple problem with two tests (t-test and
sign test) of
a one-sided comparison.

Let $X_1, ..., X_{25}$ be i.i.d. $N(0.3, 1)$.  Consider testing the null hypothesis $H_0: \mu=0$ versus $H_A: \mu>0$ at significance level $\alpha=.05$.  Compare the power of the sign test and the power of the test based on normal theory (one sample one sided t-test) assuming that $\sigma$ 
is known.

\subsection{Sign test}

We first start by calculating the Type-I error rate for the sign test.  Here we want to
reject when the number of positive values is large.  Under the null hypothesis, this is
distributed as a Binomial random variable with n=25 trials and p=0.5 probability of being
a positive value.  Let's consider values between 15 and 19.
<<pbinom>>=
xvals <- 15:19
probs <- 1 - pbinom(xvals, 25, 0.5)
cbind(xvals, probs)
@
So we see that if we decide to reject when X = the number of positive values is
17 or larger, we will have an $\alpha$ level of \Sexpr{round(1-pbinom(16, 25, 0.5), 3)},
which is near the nominal value in the problem.

We calculate the power of the sign test as follows. The probability that $X > 0$, given that $H_A$ is true is given by:
<<pnorm1>>=
1-pnorm(0, 0.3, 1)
@
We can view this graphically using the command:

\begin{center}
<<pnorm2,fig=TRUE,width=6,height=1.9>>= 
xpnorm(0, 0.3, 1)
@
\end{center}

So now we need to find the power under the alternative, which is equal to the probability of getting 17 or more positive values,
given that $p=0.6179$:
<<pbinom2>>=
1 - pbinom(16, 25, 0.6179)
@

The power is modest at best.

\subsection{T-test}

We next calculate the power of the test based on normal theory.  To keep the comparison
fair, we will set our $\alpha$ level equal to 0.05388.
First we need to find the rejection region.  

<<>>=
alpha <- 1-pbinom(16, 25, .5) # = 0.0539, our alpha level.
n <- 25; sigma <- 1 # given
stderr <- sqrt(sigma^2/n)
zstar <- qnorm(1-alpha, 0, 1)
zstar
crit <- zstar*stderr
crit
@
Therefore, we reject for observed means greater than \Sexpr{round(crit,3)}.  

To calculate the power of this one-sided test we find the area 
under the alternative hypothesis 
that is to the right of this cutoff:
<<>>=
power <- 1 - pnorm(crit, 0.3, stderr)
power
@
Thus, the power of the test based on normal theory is \Sexpr{round(power,3)}.
To provide a general check (or for future calculations of this sort) we can use the {\tt power.t.test()} in \R.
<<>>=
power.t.test(n=25, delta=.3, sd=1, sig.level=alpha, alternative="one.sided",
type="one.sample")$power
@

This yields a similar estimate to the value that we calculated directly.  
Overall, we see that the t-test has higher power than the sign test, if the underlying
data are truly normal.  It's useful to have students calculate power empirically, 
as demonstrated for this example in \ref{sec:power}.




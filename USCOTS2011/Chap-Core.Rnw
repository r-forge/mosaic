

\chapter{The Core of a Traditional Course}

\SweaveOpts{prefix.string=figures/fig}  % location of 
\SweaveOpts{highlight=F}    % not sure this does anything unless we use pgfSweave
\SweaveOpts{tidy=F}         % keep.source probably disables this
\SweaveOpts{pdf=T}          % use pdf for graphics
\SweaveOpts{strip.white=T}  % remove blank lines at beginning and end 
\SweaveOpts{keep.source=T}  % keeps formatting from original; allows ? to work
%% \SweaveOpts{eval=false}     % temporary

<<setup,echo=F>>=
source('setup.R')
@ 

In this chapter, we will briefly review the commands and functions needed
to analyze data from a more traditional introductory statistics course.
We will use data from the HELP study: a randomized trial of a novel 
way to link at-risk subjects with primary care.  More information on the
dataset can be found in section \ref{sec:help}.

Since the selection and order of topics can vary greatly from 
textbook to textbook and instructor to instructor, we have chosen to 
organize this material by the kind of data being analyzed.  This should make
it straightforward to find what you are looking for even if you present 
things in a different order.  This is also a good organizational template
to give your students to help them keep straight ``what to do when".

It's worth noting how we teach the ``Traditional Course'', which, contrary
to rumor, some of us do on occasion.  Nick has used the 5th edition of
Moore and McCabe \cite{moor:mcca:2006}, where he introduces least squares 
regression at the end of the first week of a semester, and multiple regression
(purely descriptively) at the end of the second week.  Design is the next
main topic, then a brutally pruned intro to probability and sampling distributions.
Interval estimation and testing are introduced in one or two settings, then 
the class returns to inference for multiple regression.  While students are working
on projects involving a multiple linear regression model with 2 predictors,
some categorical data analysis is covered.

\authNote{Danny and/or Randy: interested in chiming in?}

\section{One Quantitative Variable}

\subsection{Numerical Summaries}

R includes a number of commands to numerically summarize variables.
These include the capability of calculating the mean, standard deviation,
variance, median, five number summary, intra-quartile range (IQR) as well as arbitrary quantiles.  We will
illustrate these using the CESD (Center for Epidemiologic Studies - Depression)
measure of depressive symptoms.  To improve the legibility of output,
we will also set the default number of digits to display to a more reasonable
level.

<<cesd1,cache=F>>=
options(digits=3)
mean(HELP$cesd)
@
<<cesd2,cache=F>>=
sd(HELP$cesd)
@
<<cesd2b,cache=F>>=
sd(HELP$cesd)^2
@
<<cesd2c,cache=F>>=
var(HELP$cesd)
@

It is also straightforward to calculate quantiles of the distribution.

<<cesd3>>=
median(HELP$cesd)
five <- fivenum(HELP$cesd)
five     # display the object (vector of length 5)
five[4] - five[2]
IQR(HELP$cesd)
quantile(HELP$cesd)   
quantile(HELP$cesd, c(.25, .50))
@

We see that it is possible to interact with the objects created by these
functions to calculate the IQR (or a function can generate this directly).
By default, the 
\verb!quantile()! function displays the quartiles, but can be given
a vector of quantiles to display.

\subsection{Graphical Summaries}
The \verb!histogram()! function is used to create a histogram.

\vspace{-4mm}
\begin{center}
<<cesd-hist,fig=TRUE,width=5,height=1.9>>=
histogram(~ cesd, HELP)
@
\end{center}

In the HELP dataset, approximately one quarter of the subjects
are female.  It is straightforward to restrict our attention to
just those subjects.

The \verb!subset()! function can generate a new data frame containing
just the women or just the men.  Once this is created, we
used the \verb!stem()! function to create a stem and leaf plot.
<<cesd-stem>>=
female <- subset(HELP, sex=='female')
male <- subset(HELP, sex=='male')
stem(female$cesd)
@

The \verb!dotPlot()! function is used to create a dotplot (a la Fathom).

%\vspace{-8mm}
\begin{center}
<<cesd-dot,fig=TRUE,width=5,height=1.9>>=
dotPlot(~ cesd, data=female)
@
\end{center}
We could also have made our subset ``on the fly'', just for the purposes of graphing:
\begin{center}
<<cesd-dot2,fig=TRUE,width=5,height=1.9>>=
dotPlot(~ cesd, data=HELP, subset=(sex=='female'))
@
\end{center}

Or we could make side-by-side dotplots of men and women:
\begin{center}
<<cesd-dot3,fig=TRUE,width=5,height=1.9>>=
dotPlot(~cesd | sex, data=HELP)
@
\end{center}

\iffalse
Nicer if we recode:
\begin{center}
<<cesd-dot4,fig=TRUE,width=5,height=1.9>>=
HELP$sex <- factor( HELP$female, labels=c('male','female') )
dotPlot( ~ cesd | sex, data=HELP)
@
\end{center}
\fi

\subsection{Density Curves }


<<dens1,fig=TRUE,width=5,height=1.9>>=
densityplot(~ cesd, data=female)
ladd(grid.text(x=0.1, y=0.8, 'only females'))
@

\authNote{I would like to add superimposed density plots.  Is this feasible given
that we need to protect "densityplot" with a print to deal with Weave?}

\subsection{Normal Distributions}
<<norm1,fig=TRUE,width=5,height=1.9>>=
xpnorm(1.96, 0, 1)
@

\authNote{I would like to add superimposed normal on top of a density plots.  Help!}




\section{One Categorical Variable}

\subsection{Numerical Summaries}

<<homeless-table>>=
xtabs(~ homeless, HELP)
perctable(HELP$homeless)     # display table using percentages
@

\subsection{The Binomial Test}

<<binomtest>>=
binom.test(~ homeless=="housed", HELP)
@

\subsection{The Proportion Test}
<<proptest>>=
prop.test(table(HELP$homeless))
@

\authNote{suggestions for a better approach?}

\subsection{Goodness of Fit Tests}

<<gof>>=
substab = xtabs(~ substance, HELP)
substab
perctable(HELP$substance)
chisq.test(substab, p=c(1/3, 1/3, 1/3))
@


\section{Two Quantitative Variables}

\subsection{Paired t}

\verb!paired=TRUE!

\subsection{Scatterplots}

<<xyplot,fig=TRUE,width=5,height=3.9>>=
xyplot(cesd ~ mcs, data=HELP)
ladd(panel.loess(HELP$mcs, HELP$cesd))
@

\subsection{Correlation}
<<corr>>=
with(HELP, cor(cesd, mcs))
@

\subsection{Simple Linear Regression}

<<lm>>=
model <- lm(cesd ~ mcs, data=HELP)
summary(model)
@

<<lmhist,fig=TRUE,width=5,height=1.9>>=
histogram(~ residuals(model))
@

<<lmplot,fig=TRUE,width=5,height=3.9>>=
plot(model)
@

\section{Two Categorical Variables}

\subsection{Cross classification Tables}

\subsubsection{From Raw Data}
<<homeless-sex>>=
xtabs(~ homeless + sex, HELP)     
@

\subsubsection{Building Cross Tables from Already Summarized Data}

\begin{verbatim}
> mytab <- enterxtab()
Row variable name: 
1: sex
Read 1 item
Row variable levels: 
1: male
2: female
3:                 # entered a blank line here
Read 2 items
Column variable name: 
1: status
Read 1 item
Column variable levels: 
1: homeless
2: housed
3:                 # and here
Read 2 items
Row 1: 
1: 169 40
Read 2 items
Row 2: 
1: 177 67
Read 2 items
> mytab
        status
sex      homeless housed
  male        169     40
  female      177     67
\end{verbatim}

\authNote{Suggestions about how to format this?}

\subsubsection{Graphical Summaries}

<<mosaicplot,fig=TRUE,width=5,height=3.9>>=
with(HELP, mosaicplot(table(homeless, sex)))
@

\subsection{Chi-squared tests}

\subsubsection{Displaying additional information: \texttt{xchisq.test()}}

\subsection{Fisher's Exact Test}

<<help-fisher>>=
with(HELP, fisher.test(homeless, sex))
@

\section{Quantitative Response to a Categorical Predictor}

\subsection{A Bivariate Predictor: Two-sample t}
Here we will compare the distributions of CESD scores by sex.
The \verb!HELP! data set has a numeric variable \verb!female!
that codes sex us 0 for male and 1 for female.
To ensure that the display is clearly
labeled, we first create a new factor (i.e., categorical) variable called 
\verb!gender!.

<<cesd-gender-better>>=
HELP$gender <- factor(HELP$female, labels=c('male', 'female'))
# sanity check:
xtabs( ~ female + gender, HELP)
@
As it turns out, the \verb!HELP! data set already had a recoded 
version of female called \verb!sex!.  But many data sets require this sort
of conversion from numeric codes to factors.
<<>>=
# HELP already has a factor called sex, so this was redundant effort
xtabs( ~ sex + gender, HELP)
@

The \verb!tapply()! function can be used to calculate the mean CESD score
(given as first argument)
for each of the two groups (given as second argument).  The third argument
specifies which function to run for each of these subgroups.
<<tapply>>=
tapply(HELP$cesd, HELP$sex, mean)
@

\authNote{Which method(s) do we want to show?}
Alternatives to the code above:
<<tapply-alternatives>>=
aggregate(cesd ~ sex, HELP, FUN=mean)
aggregate(cesd ~ sex, HELP, FUN=favstats)
summary(cesd ~ sex, HELP, fun=mean)
summary(cesd ~ sex, HELP, fun=favstats)
@


Boxplots are a particularly helpful graphical display to compare distributions.
The \verb!bwplot()! function can be used to display the boxplots for the CESD scores separately by sex.  We see from both the numerical and graphical
displays that women tend to have slightly higher CESD scores than men.
\authNote{Should mention use of \verb!xyplot()! when the sample sizes are small.
No reason to compute 5-number summaries of 10 or fewer numbers really.}

%\vspace{-8mm}
\begin{center}
<<cesd-box,fig=TRUE,width=5,height=1.9>>=
bwplot(cesd ~ sex, data=HELP)
@
\end{center}

\subsection{Nonparametric 2 group tests}

<<help-nonpar>>=
wilcox.test(cesd ~ sex, data=HELP)
@

\subsection{1-way ANOVA}

<<help-aov>>=
mod <- aov(cesd ~ substance, data=HELP)
summary(mod)
@
<<help-aovlm>>=
mod1 <- lm(cesd ~ 1, data=HELP)
mod2 <- lm(cesd ~ substance, data=HELP)
anova(mod1, mod2)
@


\subsection{Tukey's Honest Significant Differences}

<<help-hsd>>=
with(HELP, tapply(cesd, substance, mean))
TukeyHSD(mod, "substance")
@

\section{Categorical Response to a Quantitative Predictor}

\subsection{Logistic Regression}

<<help-logit>>=
summary(glm(homeless ~ age + female, binomial, data=HELP))
@

\section{Survival Time Outcomes}

\subsection{Kaplan-Meier plot}

<<help-km, fig=TRUE,width=5,height=3.9>>=
library(survival)
fit <- survfit(Surv(dayslink, linkstatus) ~ treat, data=HELP)
plot(fit, conf.int=FALSE, lty=1:2)
@

\subsection{Cox proportional hazards model}

<<help-surv>>=
library(survival)
summary(coxph(Surv(dayslink, linkstatus) ~ age + substance, data=HELP))
@


\section{More than Two Variables}

\subsection{Two-way ANOVA}

<<help-aov2>>=
summary(aov(cesd ~ homeless + sex, data=HELP))
@


\subsection{Multiple Regression}

<<help-multreg>>=
lm1 <- lm(cesd ~ mcs + age + sex, data=HELP)
summary(lm1)
@
<<help-multreg2>>=
lm2 <- lm(cesd ~ mcs + age + sex + mcs*sex, data=HELP)
summary(lm2)
@
<<help-multreg3>>=
anova(lm1, lm2)
@

\section{Power Calculations}

\label{sec:onesamppower}

While not generally a major topic in introductory courses, power and sample size calculations
help to reinforce key ideas in statistics.  In this section, we will explore how R
can be used to undertake power calculations using analytic approaches (see \ref{sec:power}
for simulation based approaches).  We consider a simple problem with two tests (t-test and
sign test) of
a one-sided comparison.

Let $X_1, ..., X_{25}$ be i.i.d. $N(0.3, 1)$.  Consider testing the null hypothesis $H_0: \mu=0$ versus $H_A: \mu>0$ at significance level $\alpha=.05$.  Compare the power of the sign test and the power of the test based on normal theory (one sample one sided t-test) assuming that $\sigma$ 
is known.

\subsection{Sign test}

We first start by calculating the Type-I error rate for the sign test.  Here we want to
reject when the number of positive values is large.  Under the null hypothesis, this is
distributed as a Binomial random variable with n=25 trials and p=0.5 probability of being
a positive value.  Let's consider values between 15 and 19.
<<pbinom>>=
xvals = 15:19
probs = 1 - pbinom(xvals, 25, 0.5)
cbind(xvals, probs)
@
So we see that if we decide to reject when X = the number of positive values is
17 or larger, we will have an $\alpha$ level of \Sexpr{round(1-pbinom(16, 25, 0.5), 3)},
which is near the nominal value in the problem.

We calculate the power of the sign test as follows. The probability that $X > 0$, given that $H_A$ is true is given by:
<<pnorm1>>=
1-pnorm(0, 0.3, 1)
@
We can view this graphically using the command:
<<pnorm2,fig=TRUE,width=5,height=1.9>>= 
xpnorm(0, 0.3, 1)
@

So now we need to find the power under the alternative, which is equal to the probability of getting 17 or more positive values,
given that $p=0.6179$:
<<pbinom2>>=
1 - pbinom(16, 25, 0.6179)
@

The power is modest at best.

\subsection{T-test}

We next calculate the power of the test based on normal theory.  To keep the comparison
fair, we will set our $\alpha$ level equal to 0.5388.
First we need to find the rejection region.  

<<>>=
alpha <- 1-pbinom(16, 25, .5) # = 0.0539, our alpha level.
n <- 25; sigma <- 1 # given
stderr <- sqrt(sigma^2/n)
zstar <- qnorm(1-alpha, 0, 1)
zstar
crit <- zstar*stderr
crit
@
Therefore, we reject for observed means greater than \Sexpr{round(crit,3)}.  

To calculate the power of this one-sided test we find the area 
under the alternative hypothesis 
that is to the right of this cutoff:
<<>>=
power <- 1 - pnorm(crit, 0.3, stderr)
power
@
Thus, the power of the test based on normal theory is \Sexpr{round(power,3)}.
To provide a general check (or for future calculations of this sort) we can use the {\tt power.t.test()} in R.
<<>>=
power.t.test(n=25, delta=.3, sd=1, sig.level=alpha, alternative="one.sided",
type="one.sample")$power
@

This yields a similar estimate to the value that we calculated directly.  
Overall, we see that the t-test has higher power than the sign test, if the underlying
data are truly normal.  It's useful to have students calculate power empirically, 
as demonstrated for this example in \ref{sec:power}.

\section{Design and Randomization}  

\authNote{Drop this?}

\section{Bias and Variability}

\authNote{Drop this?}

\section{Probability and Random Variables}

\label{sec:DiscreteDistributions}



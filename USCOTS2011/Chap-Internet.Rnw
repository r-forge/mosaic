\chapter{Taking Advantage of the Internet}


\SweaveOpts{prefix.string=figures/fig}  % location of 
\SweaveOpts{highlight=T}    % not sure this does anything unless we use pgfSweave
\SweaveOpts{tidy=F}         % keep.source probably disables this
\SweaveOpts{pdf=T}          % use pdf for graphics
\SweaveOpts{strip.white=T}  % remove blank lines at beginning and end 
\SweaveOpts{keep.source=T}  % keeps formatting from original; allows ? to work

<<setup,echo=F>>=
source('setup.R')
@ 

Provocative papers by Gould \cite{Goul:2010} and Nolan and Temple Lang \cite{nola:temp:2010}
have highlighted the importance of broadening the type of data students encounter
in their first courses as well as the role of computing in modern statistics, respectively.
In this chapter, we highlight some approaches which facilitate use of data scraping
from the internet as well as interfaces to web based tools to achieve both these goals.

\section{Google Docs and \RStudio}

\authNote{Move to appendix B?}

\section{Sharing in \RStudio}

\authNote{Move to appendix B?}

\section{Other Ways to Make Data Available Online}

\authNote{Move to appendix B?}

\section{Data Mining Activities}
\subsection{What percentage of Earth is Covered with Water?}
\label{sec:googleMap}
We can estimate the proportion of the world covered with water by randomly 
sampling points on the globe and inspecting them using GoogleMaps.

First, let's do a sample size computation.  Suppose we want to 
estimate (at the 95\% confidence level) this proportion with $\pm 5$\%.
There are several ways to estimate the necessary sample size, including
algebraically solving
\[
(1.96) \sqrt{ \hat p (1-\hat p) /n} = 0.05
\]
for $n$ given some estimated value of $p$.  The \verb!uniroot()! function
can solve this sort of thing numerically.  Here we take an approach 
that looks at a table of values of $n$ and $\hat p$ and margin of error.
<<google-sample-size>>=
n <- seq(50,500, by=50)
p.hat <- seq(.5, .9, by=0.10)
margin_of_error <- function(n, p, conf.level=.95) { 
	-qnorm( (1-conf.level)/2) * sqrt( p * (1-p) / n ) 
}
outer(n, p.hat, margin_of_error) -> tbl
colnames(tbl) <- p.hat
rownames(tbl) <- n
tbl
@
From this it appears that a sample size of approximately 300--400 will get
us the accuracy we desire.  A class of students can easily generate
this much data in a matter of minutes if each student inspects 10--20 maps.
The example below assumes a sample size of 10 locations per student.
This can be adjusted depending on the number of students and the desiered
margin of error.

\begin{enumerate}
\item Generate 10 random locations.

<<rgeo>>=
positions <- rgeo(10); positions
@

\item
Open a GoogleMap centered at each position.

<<googleMap,eval=false>>=
googleMap(pos=positions, mark=TRUE)
@
You may need to turn off pop-up block for this to work smoothly.

\item
For each map, record whether the center is located in water or on land.  \verb!mark=TRUE!
is used to place a marker at the center of the map which is helpful for locations that are close to 
the coast.  
\begin{center}
\includegraphics[width=.8\textwidth]{images/google-water1}
\end{center}
You can zoom in or out to get a better look.
\begin{center}
\includegraphics[width=.8\textwidth]{images/google-water2}
\end{center}


\item
Record your data in a GoogleForm at 

\begin{center}
\url{https://spreadsheets.google.com/viewform?formkey=dGREcUR6YjRLSWFTWVpNNXA5ZUZ1TXc6MQ}

\includegraphics[width=.4\textwidth]{images/googleForm-water}
\end{center}

For the latitude and longitude information, simply copy and paste the output of 
<<googleMap-positions,eval=false>>=
positions
@
\item
After importing the data from Google, it is simple to sum the counts across the class.

<<sum-water-setup,echo=false>>=
Water = data.frame(Water=215, Land=85)
@

<<sum-water>>=
sum(Water$Water)
sum(Water$Land)
@

Then use your favorite method of analysis, perhaps \verb!binom.test()!.

<<googleMap-binom.test>>=
interval(binom.test(215,300))
@
\end{enumerate}


\subsection{Roadless America}

The \verb!rgeo()! can also sample within a latitude longitude ``rectangle".
This allows us to sample subsets of the globe.  In this activity we will estimate 
the proportion of the continental United States that is within 1 mile of a road.

\begin{enumerate}
\item
Generate a random sample of locations in a box containing the continental United States.
Some of these points may be in Canada, Mexico, an ocean or a major lake.  These 
will be discarded from our sample before making our estimate.
<<rgeo-roadless>>=
positions <- rgeo(10, lonlim=c(-125,-65), latlim=c(25,50)); positions
@

\item
Open a GoogleMap centered at each position.  This time we'll zoom in a bit and add 
a circle of radius 1 to our map.

<<googleMap-roadless,eval=false>>=
googleMap(pos=positions, mark=TRUE, zoom=12, radius=1)
@


\begin{center}
\includegraphics[width=.8\textwidth]{images/google-roadless}
\end{center}
You may need to turn off pop-up block for this to work smoothly.
\item
For each map, record whether the center is close (to a road), far (from a road), water, or foreign.
You may need to zoom in or out a bit to figure this out.

\end{enumerate}

\subsection{Variations on the Google Maps theme}

There are many other quantities one could estimate using these tools.  For example:
\begin{enumerate}
\item
What proportion of you home state is within $m$ miles of a lake?  (The choice of $m$ may depend upon
your state of interest.)
\item
Use two proportion procedures  or chi-squared tests to compare states or continents.  
Do all continents have roughly the same proportion of land withing $m$ miles of water (for some $m$)?
Are Utah and Arizona equally roadless?

\item
In more advanced classes: What is the average distance to the nearest lake (in some region)?
By using concentric circles, one could estimate this from discretized data indicating, for example,
whether the nearest lake is within 1/2 mile, between 1/2 mile and 1 mile, between 1 mile and 2 miles,
between 2 miles, and 4 miles, between 4 miles and 10 miles, or more than 10 miles away.  It may be 
interesting to discuss what sort of model should be used for distances from random locations to lakes.
(It probably isn't normally distributed.)
\authNote{Is this example too complicated?}%
\end{enumerate}

\subsection{Zillow}

Zillow.com is an online real estate database that can be used to estimate
property values using tax records, sales data, and comparable homes.  

\centerline{\includegraphics[width=3.8in]{images/zillow1.png}}

They have made an application programming interface (aka API) that specify 
how software programs can interface with their system.  Duncan Temple Lang has
crafted a package in R which talks to Zillow. 
This can be used to dynamically generate datasets for use in courses, after
you (and/or your students) generate a \verb!zillowId! for use with the system.
(Danny Kaplan has used {\tt cars.com} to similar ends).

In this section, we describe how to use Zillow to generate and analyse a
dataset comprised of comparable sites to an arbitrary house of interest.

The first step is to create a Zillow account (click on \verb!Register! on the
top right of the page at \verb!zillow.com!).  You can set up an account or register
using Facebook.

Once you have the account, log in, then click on \verb!My Zillow! at the top right.
This should display your profile (in this case, for a user named \verb!SC_z!).

\centerline{\includegraphics{images/zillow_profile.pdf}}

Next, 
open the page: \url{http://www.zillow.com/webservice/Registration.htm}.  This
is the application programming interface (API) request, which requires more information
if you are a real estate professional.  Note that there are limits on the use 
of these data, which at first glance appear to not contravene use for statistics
activities and data collection. An overview of the API and terms of use can be found
at \url{http://www.zillow.com/howto/api/APIOverview.htm}.

\centerline{\includegraphics[width=4.6in]{images/zillow_api.pdf}}

You should receive information about your Zillow ID (a character string
of letters and numbers).  

Once you've set up your Zillow account, and gotten your Zillow Id,
the next step is to install the Zillow package. Unlike others described
to date, this is available on the Omegahat server (a different repository than CRAN):
<<zillowinst,echo=TRUE, eval=FALSE>>=
install.packages("Zillow", repos="http://www.omegahat.org/R", type="source")
@

Next, you should
initialize your \verb!zillowID! to the value that you 
received when you registered with {\tt Zillow.com}.
<<fakezillow>>=
zillowId = "set_to_your_zillowId"
@
<<zillowid, eval=TRUE, echo=FALSE>>=
zillowId = "X1-ZWz1bvi5ru1gqz_4srxq"  # this is Nick's, please don't share!
@

This allows you to make calls to functions such as \verb!zestimate()! (which
allows you to search for information about a particular property) and
\verb!getComps()! (which facilitates finding a set of comparable properties.  
Here we find information about an arbitrary house in California, as well as
comparable properties.
<<zillow1>>=
if(require(Zillow)) {
  est = zestimate("1280 Monterey Avenue", "94707", zillowId)
  comps = getComps(rownames(est), zillowId)
}
if(require(Zillow)) {
  est
}
if(require(Zillow)) {
  rownames(est)
}
if(require(Zillow)) {
  names(comps)
}
@
<<zillow2>>=
if(require(Zillow)) {
  table(comps$bathrooms)
}
if(require(Zillow)) {
  table(comps$bedrooms)
}
if(require(Zillow)) {
  fivenum(comps$finishedSqFt)
}
@
\authNote{Should I use some variant of bwplot instead?}
<<zillowprices,fig=TRUE>>=
if(require(Zillow)) {
  boxplot(comps$low, comps$taxAssessment, comps$high, names=c("Low", "Assessed", "High"))
} else {
  plot(1:10)
}
@

It's interesting that for this property, assessed values tend to lag far behind the lower and
upper Zillow estimates.  We could explore whether this is true in California more generally.

It's possible to plot the results of our comparable properties, which yields a scatterplot
of price by square feet (with the number of bedrooms as well as the low and high range) as
well as a scatterplot of amount/finished square feet vs log size in square feet.
<<zillowcomps,fig=TRUE>>=
if(require(Zillow)) {
  plot(comps)
} else {
  plot(1:10)
}
@

Several aspects of this activity are worth noting: 
\begin{enumerate}
\item There is some startup cost for instructors and students (since each user will need
their own ZillowID\footnote{By default, the number of calls per day to the API is limited to 1000,
which could easily be exceeded in a lab setting if, contrary to the terms of use, the Zillow ID
were to be shared.}).  
\item Once set up, the calls to the Zillow package are very straightforward, and provide
immediate access to a wealth of interesting data.
\item This could be used as an activity to provide descriptive analysis of comparable 
properties, or in a more elaborate manner to compare properties in different cities or
areas.
\item Since the latitude and longitude of the comparable properties is returned, users
can generate maps using the mechanisms described in \ref{sec:googleMap}.
\end{enumerate}

\subsection{Other variants}

We've outlined several approaches which efficiently scrape data from the web.
But there are lots of other examples (many due to Duncan Temple Lang) which may be worth exploring.  These include:
\begin{description}
\item[NY Times:] interface to several of the \emph{New York Times} web services
for searching articles, meta-data, user-generated content and best seller lists 
(\url{http://www.omegahat.org/RNYTimes})
\item[Flickr:] interface to the Flickr photo sharing service 
(\url{http://www.omegahat.org/Rflickr})
\item[Google Docs:] interface to allow listing documents on Google Docs along with details,
downloading contents, and uploading files (\url{http://www.omegahat.org/RGoogleDocs})

\item[Twitter:] interface to access Twitter feeds in various ways 
(\url{http://cran.r-project.org/web/packages/twitteR})
\item[last.fm:] interface to the \verb!last.fm! music recommendation site (\url{http://cran.r-project.org/web/packages/RLastFM})
\end{description}





